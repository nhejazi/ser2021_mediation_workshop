# Estimation of natural (in)direct effects, interventional (in)direct effects

## Natural direct and indirect effects

Recall:

```{tikz, fig.cap = "Directed acylcic graph under *no intermediate confounders* of the mediator-outcome relation affected by treatment", fig.ext = 'png', cache = TRUE, echo = FALSE}
\dimendef\prevdepth=0
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
\usetikzlibrary{arrows,positioning}
\tikzset{
>=stealth',
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}
\newcommand{\Vertex}[2]
{\node[minimum width=0.6cm,inner sep=0.05cm] (#2) at (#1) {$#2$};
}
\newcommand{\VertexR}[2]
{\node[rectangle, draw, minimum width=0.6cm,inner sep=0.05cm] (#2) at (#1) {$#2$};
}
\newcommand{\ArrowR}[3]
{ \begin{pgfonlayer}{background}
\draw[->,#3] (#1) to[bend right=30] (#2);
\end{pgfonlayer}
}
\newcommand{\ArrowL}[3]
{ \begin{pgfonlayer}{background}
\draw[->,#3] (#1) to[bend left=45] (#2);
\end{pgfonlayer}
}
\newcommand{\EdgeL}[3]
{ \begin{pgfonlayer}{background}
\draw[dashed,#3] (#1) to[bend right=-45] (#2);
\end{pgfonlayer}
}
\newcommand{\Arrow}[3]
{ \begin{pgfonlayer}{background}
\draw[->,#3] (#1) -- +(#2);
\end{pgfonlayer}
}
\begin{tikzpicture}
  \Vertex{-4, 0}{W}
  \Vertex{0, 0}{M}
  \Vertex{-2, 0}{A}
  \Vertex{2, 0}{Y}
  \Arrow{W}{A}{black}
  \Arrow{A}{M}{black}
  \Arrow{M}{Y}{black}
  \ArrowL{W}{Y}{black}
  \ArrowL{A}{Y}{black}
  \ArrowL{W}{M}{black}
\end{tikzpicture}
```

Assuming a binary $A$, we define the natural direct effect as:
$$NDE = E(Y_{1,M_{0}} - Y_{0,M_{0}})$$,

and the natural indirect effect as:
$$NIE = E(Y_{1,M_{1}} - Y_{1,M_{0}})$$.

### Simple case for intuition

The observed data is
\[O=(W, A, M, Y)\]

This SCM is represented in the above DAG and the following causal models:
\[
\begin{align*}
W & = f_W(U_W)\\
A & = f_A(W, U_A)\\
M & = f_M(W, A, U_M)\\
Y & = f_Y(W, A, M, U_Y),
\end{align*}
\]
where $(U_W, U_A,U_M, U_Y)$ are exogenous random errors.

We assume
- $A$ is a single binary randomized treatment (and thus $A & = f_A(U_A)$)
- $M$ is a single binary mediator
- There are no restrictions on the distribution of $W$ or $Y$

Recall that we need to assume the following to identify the above caual effects
from our observed data:

- $A \indep Y_{a,m} \mid W$
- $M \indep Y_{a,m} \mid W, A$
- $A \indep M_a \mid W$
- $M_0 \indep Y_{1,m} \mid W$
- and positivity assumptions

### How to estimate using g-computation

<!--
ID: Should we add the g-computation formula here?
-->

Let's take the NDE as an example:

1. Fit a regression of $Y$ on $M,A,W$. Predict outcome values setting $A=1$.
   We'll call the result $\bar{Q}_Y(M,1,W)$. Predict outcome values setting
   $A=0$. We'll call the result $\bar{Q}_Y(M,0,W)$.
2. Take the difference $\bar{Q}_Y(M,1,W) - \bar{Q}_Y(M,0,W)$ and regress it on
   $W$ among those for whom $A=0$. This recovers the expected difference had all
   individuals been set to the control condition $A = 0$.
3. The sample mean of the predicted values gives the estimate.

### How to estimate using the doubly robust methods that rely on the EIF

The EIC for the NDE ($\Psi_{NDE}$) is given by:

<!--
ID: I'd suggest using the notation in our biometrika paper for the below
-->

\begin{align}
    D^{\star} &= \bigg\{ \frac{I(A=1)}{g(1|W)}\frac{Q(M|W,0)}{Q(M|W,1)} -
      \frac{I(A=0)}{g(0|W)}\bigg\} \times (Y-\bar{Q}_Y(M,A,W))  \\
    &+ \frac{I(A=0)}{g(0|W)}\{ \bar{Q}_{diff} - E(\bar{Q}_{diff} | W,0) \}\\
    &+ E(\bar{Q}_{diff} | W,0) - \Psi_{NDE}
\end{align}

### How to estimate using TMLE

1. Estimate
  \begin{equation*}
    C_Y(Q_M, g)(O) = \Bigg\{\frac{\I(A = 1)}{g(1 \mid W)}
      \frac{Q_M(M \mid 0, W)}{Q_M(M \mid 1, W)} -
      \frac{\I(A = 0)}{g(0 \mid W)} \Bigg\}.
  \end{equation*}
Breaking this down, $\frac{\I(A = 1)}{g(1 \mid W)}$ is the inverse probability
weight for $A = 1$ and, likewise, $\frac{\I(A = 0)}{g(0 \mid W)}$ is the inverse
probability weight for $A = 0$. The middle term is the ratio of the mediator
density when $A = 0$ to the mediator density when $A = 1$.

Estimating $Q_M$ is a really hard problem when $M$ is high-dimensional. But,
since we have the ratio of these conditional densitities, we can reparamterize
using Bayes rule to get something that is easier to compute:
\begin{equation*}
  \frac{\P(A = 0 \mid M, W) g(0 \mid W)}{\P(A = 1 \mid M, W) g(1 \mid W)}.
\end{equation*}

Underneath the hood, the counterfactual outcome difference
$\bar{Q}_{\text{diff}}$ and $P(A \mid Z, W)$, the conditional probability of $A$
given $Z$ and $W$, are used in constructing the auxiliary covariate for TML
estimation. These nuisance parameters play an important role in the
bias-correcting _TMLE-update step_.

1. We estimate $g_{A \mid W}(W)=P(A=a \mid W)$ from a logistic regression of
   $A$ on $W$, generating predicted probabilities that $A=1$ for $g(1 \mid W)$
   and $A=0$ for $g(0 \mid W)$.  
2. We estimate $\P(A=a \mid M, W)$ from a logistic regression of $A$ on $M, W$,
   generating predicted probabilities that $A=1$ for and $A=0$.

```{r, echo=TRUE,eval=FALSE}
amodel <- "a ~ w "
mmodel <- "m ~ a + w"
amodel <- "a ~ m + w"
ymodel <- "y ~ m + a*w"

# make gm
afit <- glm(formula = amodel, family = "binomial", data = obsdat)
mfit <- glm(formula = mmodel, family = "binomial", data = obsdat)

a1 <- predict(afit, newdata = data.frame(w = obsdat$w), type = "response")
a0 <- 1 - a1

am1 <- predict(amfit,
  newdata = data.frame(w = obsdat$w, m = obsdat$m),
  type = "response"
)
am0 <- 1 - am1
cy <- (am0 * a0) / (am1 * a1)
```

$\bar{Q}_Y(M,a^\prime,W) - \bar{Q}_Y(M,a^\star,W)$

3. To obtain an estimate of $\bar{Q}_{diff} = \bar{Q}_Y(M,1,W) -
   \bar{Q}_Y(M,0,W)$, predict values of $Y$ from a regression of $Y$ on $M,A,W$,
   setting $A=1$ and $A=0$, giving $\hat{Y}(m, 1, w)$ and $\hat{Y}(m, 1, w)$.

```{r, echo=TRUE,eval=FALSE}
qyinit <- cbind(
  predict(glm(
    formula = ymodel, family = "binomial",
    data = data.frame(cbind(datw, a = a, m = m, y = y))
  ),
  newdata = data.frame(cbind(datw, a, m)), type = "response"
  ),
  predict(glm(
    formula = ymodel, family = "binomial",
    data = data.frame(cbind(datw, a = a, m = m, y = y))
  ),
  newdata = data.frame(cbind(datw, a = 0, m)), type = "response"
  ),
  predict(glm(
    formula = ymodel, family = "binomial",
    data = data.frame(cbind(datw, z = z, m = m, y = y))
  ),
  newdata = data.frame(cbind(datw, a = 1, m)), type = "response"
  )
)

qbardiff <- qyinit[, 3] - qyinit[, 2]
```

4. Estimate $\hat{\epsilon}$ by setting $\epsilon$ as the intercept of a
   weighted logistic regression model of $Y$ with
   $logit(\hat{\bar{Q}}_{Y}(M,A,W))$ as an offset and weights $\hat{C}_{Y}$.

5. The estimates of $\bar{Q}_{Y}(M,1,W)$ and $\bar{Q}_{Y}(M,0,W)$ are updated
   by $\hat{\bar{Q}}^{\star}_{Y}(M,A,W) =
   \hat{\bar{Q}}_{Y}(\epsilon_n)(M,A,W)$. This gives an updated difference:
   $\hat{\bar{Q}}^{\star}_{diff}(M,A,W)$.

```{r, echo=TRUE,eval=FALSE}
epsilon <- coef(glm(y ~ 1,
  weights = cy, offset = (qlogis(qyinit[, 1])),
  family = "quasibinomial"
))
qyupa0 <- plogis(qlogis(qyinit[, 2]) + epsilon)
qyupa1 <- plogis(qlogis(qyinit[, 3]) + epsilon)
qdiffup <- qyupa1 - qyupa0
```

6. We then regress $\hat{\bar{Q}}^{\star}_{diff}(M,A,W)$ on $W$ among those
   with $A=0$. Taking the empirical mean of the predicted values gives us the
   TML estimate of the NDE.

```{r, echo=TRUE,eval=FALSE}
margqdiff_fit <- glm(qdiffup ~ w,
  data = data.frame(
    qdiffup = qdiffup[a == 0],
    w = w[a == 0]
  )
)
margqdiff <- predict(margqdiff_fit,
  newdata = data.frame(qdiffup = qdiffup, w = w)
)
tmlende <- mean(margqdiff)
```

## Interventional direct and indirect effects

Recall that in the presence of a intermediate confounder natural (in)direct effects are not identified

```{tikz, fig.cap = "Directed acylcic graph under intermediate confounders of the mediator-outcome relation affected by treatment", fig.ext = 'png', cache = TRUE, echo = FALSE}
\dimendef\prevdepth=0
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
\usetikzlibrary{arrows,positioning}
\tikzset{
>=stealth',
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}
\newcommand{\Vertex}[2]
{\node[minimum width=0.6cm,inner sep=0.05cm] (#2) at (#1) {$#2$};
}
\newcommand{\VertexR}[2]
{\node[rectangle, draw, minimum width=0.6cm,inner sep=0.05cm] (#2) at (#1) {$#2$};
}
\newcommand{\ArrowR}[3]
{ \begin{pgfonlayer}{background}
\draw[->,#3] (#1) to[bend right=30] (#2);
\end{pgfonlayer}
}
\newcommand{\ArrowL}[3]
{ \begin{pgfonlayer}{background}
\draw[->,#3] (#1) to[bend left=45] (#2);
\end{pgfonlayer}
}
\newcommand{\EdgeL}[3]
{ \begin{pgfonlayer}{background}
\draw[dashed,#3] (#1) to[bend right=-45] (#2);
\end{pgfonlayer}
}
\newcommand{\Arrow}[3]
{ \begin{pgfonlayer}{background}
\draw[->,#3] (#1) -- +(#2);
\end{pgfonlayer}
}
\begin{tikzpicture}
  \Vertex{0, -1}{Z}
  \Vertex{-4, 0}{W}
  \Vertex{0, 0}{M}
  \Vertex{-2, 0}{A}
  \Vertex{2, 0}{Y}
  \ArrowR{W}{Z}{black}
  \Arrow{Z}{M}{black}
  \Arrow{W}{A}{black}
  \Arrow{A}{M}{black}
  \Arrow{M}{Y}{black}
  \Arrow{A}{Z}{black}
  \Arrow{Z}{Y}{black}
  \ArrowL{W}{Y}{black}
  \ArrowL{A}{Y}{black}
  \ArrowL{W}{M}{black}
\end{tikzpicture}
```

<!--
ID: I suggest using the Y_{a, G_a'} notation I used in the other page to define these effects
-->

We define the interventional direct effect as:
\begin{equation*}
  \psi_{\text{PIDE}} = \E(Y_{a^\prime,g_{M \mid a^\star,W}} -
    Y_{a\star,g_{M \mid a^\star,W}}),
\end{equation*}

and the interventional indirect effect as:

\begin{equation*}
  \psi_{\text{PIIE}} = \E(Y_{a^\prime,g_{M \mid a^\prime,W}} -
    Y_{a^\prime,g_{M \mid a^\star,W}}).
\end{equation*}

## Simple case for intuition

Consider a simple data structure $O=(W, A, Z, M, Y)$. This SCM is represented in
the above DAG and the following causal models:

\[
\begin{align*}
W & = f_W(U_W)\\
A & = f_A(W, U_A)\\
Z & = f_Z(W, A, U_Z)\\
M & = f_M(W, A, Z, U_M)\\
Y & = f_Y(W, A, Z, M, U_Y)
\end{align*}
\]

where ($U_W, U_A, U_Z, U_M, U_Y$) are exogenous random errors. We assume $A$ is
a single binary treatment, $Z$ is a single binary intermediate confounder, $M$
is a single binary mediator. There are no restrictions on the distribution of
$W$ or $Y$.

$g_{M \mid a^\prime,W}$ represents a stochastic draw from the counterfactual,
conditional distribution of $M$, as described by
@vanderweele2016mediation:

<!--
ID: The below is not a stochastic draw, it is the distribution that you are drawing from. Also, not sure about that last equality, isn't that just part of the identification strategy, of which we have not talked about? Probably confusing to put that in here.
-->

\begin{equation*}
  g_{M \mid A,W}(m, a^{\star}, W) \equiv g_{M \mid a^{\star}, W}(W) =
    \sum_{z=0}^1 \P(M=1 \mid Z=z,W) \P(Z=z \mid A=a^{\star}, W).
\end{equation*}

<!--
ID: Why would we make the below assumption if we have estiamtors that do not require it? It will probably just confuse people to say that we make this assumption and then have software that does not make that assumption
-->

In what follows, we are going to assume that $g_{M \mid A,W}(m, a^{\star}, W)$
is known, estimated from observed data, which we call
$\hat{g}_{M \mid a^{\star}, W}$. This is going to slightly alter the usual
identification assumptions such that we no longer need to assume exchangeability
of $A$ and the counterfactual $M$ values.  This means the remaining assumptions
are the same as those for controlled direct effects.

### Estimation using G-Computation

The estimand $E(Y_{a^\prime, \hat{g}_{M \mid a^\star,W}})$ can be identified
via sequential regression, which provides the framework for the
G-computation-based estimator. The procedure is as follows

1. Fit a regression of $Y$ on $M,Z,W$. Predict outcome values under under
   $M=m$. We'll call the result $\bar{Q}_Y(M,Z,W)$.
2. Integrate out $M$ under our stochastic intervention
   $\hat{g}_{M \mid a^{\star}, W}$. We can do this by evaluating
   $\E(Y \mid M=m,Z=z,W)$ at each $m$ and multiplying it by the probability
   that $M=m$ under $\hat{g}_{M \mid a^{\star}, W}$, summing over all $m$.
   We'll call the results $\bar{Q}^{g}_M(Z,W)$.
3. Integrate out $Z$ and set $A=a^\prime$. Again, we can do this by evaluating
   the predicted values from Step 2, setting $A=a^\prime$, and at each $z$,
   multiplying the prediction by the probability that $Z=z$ under $A=a^\prime$.
   We'll call the result $\bar{Q}^{a^\prime}_Z(W)$.
4. Taking the sample mean (marginalizing over $W$) gives the parameter
   estimate.

### Estimate with doubly robust methods based on the EIF

The EIF for the parameter $\Psi(P)(a^{\prime}, \hat{g}_{M \mid a^{\star},W})$,
where, again, $\hat{g}_{M \mid a^{\star}, W}$ is assumed known, is given by:
\begin{align*}
  D^{\star}(a^{\prime}, \hat{g}_{M \mid a^{\star}, W}) &= \sum_{k=0}^2
      D_k^{\star}(a^{\prime}, \hat{g}_{M \mid a^{\star}, W}), \text{ where }\\
  D^{\star}_0(a^{\prime}, \hat{g}_{M \mid a^{\star}, W}) &=
      \bar{Q}^{a^{\prime}}_{Z(W)} -
      \Psi(P)(a^{\prime}, \hat{g}_{M \mid a^{\star}, W})\\
  D^{\star}_1(a^{\prime}, \hat{g}_{M \mid a^{\star}, W}) &=
      \frac{I(A=a^{\prime})}{\P(A=a^{\prime} \mid W)}(\bar{Q}^{\hat{g}}_M(Z,W)
      - \bar{Q}^{a^{\prime}}_{Z(W)})\\
  D^{\star}_2(a^{\prime}, \hat{g}_{M \mid a^{\star}, W}) &=
      \frac{I(A=a^{\prime})\{I(M=1) \hat{g}_{M \mid a^{\star}, W} +
      I(M=0)(1-\hat{g}_{M \mid a^{\star}, W}) \}}{\P(A=a^{\prime}}
      &\times (Y-\bar{Q}_{Y(M,Z,W)}).
\end{align*}

### Estimate using TMLE

1. We estimate $g_{Z \mid a^{\star}, W}(W) = \P(Z=1 \mid A=a^{\star}, W)$ from
   a logistic regression of $Z$ on $A, W$ setting $A=a^{\star}$.
2. We then estimate $g_{M \mid z,W}(W) = \P(M=1 \mid Z=z, W)$ from a logistic
   regression of $M$ on $Z, W$, setting $z=\{0,1\}$.
3. We use these quantities to calculate $\hat{g}_{M \mid a^{\star}, W} =
   \hat{g}_{M \mid z=1,W}\hat{g}_{Z \mid a^{\star}, W} +
   \hat{g}_{M \mid z=0,W}(1-\hat{g}_{Z|a^{\star}, W})$.

```{r, eval=FALSE}
zmodel <- "z ~ a + w1 "
mmodel <- "m ~ z + w1"
ymodel <- "y ~ m + z*w1"

# make gm and get counterfactual predictions
zfit <- glm(formula = zmodel, family = "binomial", data = obsdat)
mfit <- glm(formula = mmodel, family = "binomial", data = obsdat)

za0 <- predict(zfit,
  newdata = data.frame(w1 = obsdat$w1, a = 0),
  type = "response"
)
za1 <- predict(zfit,
  newdata = data.frame(w1 = obsdat$w1, a = 1),
  type = "response"
)

mz1 <- predict(mfit,
  newdata = data.frame(w1 = obsdat$w1, z = 1),
  type = "response"
)
mz0 < -predict(mfit,
  newdata = data.frame(w1 = obsdat$w1, z = 0),
  type = "response"
)

gm0 <- (mz1 * za0) + (mz0 * (1 - za0))
gma1 <- (mz1 * za1) + (mz0 * (1 - za1))
```

4. To obtain an estimate of $\bar{Q}_{Y}(M,Z,W)$, predict values of $Y$ from a
   regression of $Y$ on $M,Z,W$, setting $m=1$ and $m=0$, giving
   $\hat{Y}(m=1, z, w)$ and $\hat{Y}(m=0, z, w)$.

```{r, eval=FALSE}
tmpdat$qyinit <- cbind(
  predict(glm(
    formula = ymodel, family = "binomial",
    data = data.frame(cbind(datw, z = z, m = m, y = y))
  ),
  newdata = data.frame(cbind(datw, z = z, m = m)), type = "response"
  ),
  predict(glm(
    formula = ymodel, family = "binomial",
    data = data.frame(cbind(datw, z = z, m = m, y = y))
  ),
  newdata = data.frame(cbind(datw, z = z, m = 0)), type = "response"
  ),
  predict(glm(
    formula = ymodel, family = "binomial",
    data = data.frame(cbind(datw, z = z, m = m, y = y))
  ),
  newdata = data.frame(cbind(datw, z = z, m = 1)), type = "response"
  )
)
```

5. Estimate the weights to be used for the initial targeting step:
   \begin{equation*}
      h_1(a) = \frac{I(A=a)\{I(M=1)\hat{g}_{M \mid a^{\star}, W} +
        I(M=0)(1-\hat{g}_{M \mid a^{\star}, W}) \}}{\P(A=a)\{I(M=1)
        g_{M \mid Z,W} + I(M=0)(1-g_{M \mid Z,W}) \}}
   \end{equation*}

```{r, eval=FALSE}
psa1 <- I(a == 1) / mean(a)
psa0 <- I(a == 0) / mean(1 - a)
mz <- predict(glm(
  formula = mmodel, family = "binomial",
  data = data.frame(cbind(datw, z = z, m = m))
),
newdata = data.frame(cbind(datw, z = z)), type = "response"
)
psm <- (mz * m) + ((1 - mz) * (1 - m))

tmpdat$ha1gma1 <- ((m * gma1 + (1 - m) * (1 - gma1)) / psm) * psa1 * svywt
tmpdat$ha1gma0 <- ((m * gm + (1 - m) * (1 - gm)) / psm) * psa1 * svywt
tmpdat$ha0gma0 <- ((m * gm + (1 - m) * (1 - gm)) / psm) * psa0 * svywt
```

6. Estimate $\hat{\epsilon}$ by setting $\epsilon$ as the intercept of a
   weighted logistic regression model of $Y$ with
   $\text{logit}(\hat{\bar{Q}}_{Y}(M,Z,W))$ as an offset and weights
   $\hat{h}_{1}(a)$. (Note that this is just one possible TMLE.)

7. The estimate of $\bar{Q}_{Y}(M,Z,W)$ is updated by
   $\hat{\bar{Q}}^{\star}_{Y}(M,Z,W) =  \hat{\bar{Q}}_{Y}(\epsilon_n)(M,Z,W)$.

```{r, eval=FALSE}
# for E(Y_{1,gmastar})
epsilonma1g0 <- coef(glm(y ~ 1,
  weights = tmpdat$ha1gma0,
  offset = (qlogis(qyinit[, 1])),
  family = "quasibinomial", data = tmpdat
))
tmpdat$qyupm0a1g0 <- plogis(qlogis(tmpdat$qyinit[, 2]) + epsilonma1g0)
tmpdat$qyupm1a1g0 <- plogis(qlogis(tmpdat$qyinit[, 3]) + epsilonma1g0)

# for E(Y_{1,gma})
epsilonma1g1 <- coef(glm(y ~ 1,
  weights = tmpdat$ha1gma1,
  offset = (qlogis(qyinit[, 1])),
  family = "quasibinomial", data = tmpdat
))
tmpdat$qyupm0a1g1 <- plogis(qlogis(tmpdat$qyinit[, 2]) + epsilonma1g1)
tmpdat$qyupm1a1g1 <- plogis(qlogis(tmpdat$qyinit[, 3]) + epsilonma1g1)

# for E(Y_{0,gmastar})
epsilonma0g0 <- coef(glm(y ~ 1,
  weights = tmpdat$ha0gma0,
  offset = (qlogis(qyinit[, 1])),
  family = "quasibinomial", data = tmpdat
))
tmpdat$qyupm0a0g0 <- plogis(qlogis(tmpdat$qyinit[, 2]) + epsilonma0g0)
tmpdat$qyupm1a0g0 <- plogis(qlogis(tmpdat$qyinit[, 3]) + epsilonma0g0)
```

8. We next integrate out $M$ from $\bar{Q}^{\star}_{Y}(M,Z,W)$. First, we
   estimate $\bar{Q}^{\star}_{Y,n}(M,Z,W)$ setting $m=1$ and $m=0$, giving
   $\bar{Q}^{\star}_Y(m=1, z, w)$ and $\bar{Q}^{\star}_Y(m=0, z, w)$. Then,
   multiply these predicted values by their probabilities under
   $\hat{g}_{M \mid a^{\star},W}(W)$ (for $a \in \{a, a^{\star}\}$), and add
   them together (i.e., $\bar{Q}^{\hat{g}}_{M,n}(Z,W) =
   \hat{Q}^{\star}_Y(m=1, z, w) \hat{g}_{M|a^{\star},W} +
   \hat{Q}^{\star}_Y(m=0, z, w)(1-\hat{g}_{M|a^{\star},W})$).

```{r, eval=FALSE}
tmpdat$Qma1g0 <- tmpdat$qyupm0a1g0 * (1 - gm) + tmpdat$qyupm1a1g0 * gm
tmpdat$Qma1g1 <- tmpdat$qyupm0a1g1 * (1 - gma1) + tmpdat$qyupm1a1g1 * gma1
tmpdat$Qma0g0 <- tmpdat$qyupm0a0g0 * (1 - gm) + tmpdat$qyupm1a0g0 * gm
```

9. We now fit a regression of $\bar{Q}^{\hat{g},\star}_{M,n}(Z,W)$ on $W$
   among those with $A=a^\prime$. We call the predicted values from this
   regression $\hat{\bar{Q}}^{a^\prime}_{Z}(W)$.

```{r, eval=FALSE}
Qzfita1g0 <- glm(
  formula = paste("Qma1g0", qmodel, sep = "~"),
  data = tmpdat[tmpdat$a == 1, ], family = "quasibinomial"
)
Qzfita1g1 <- glm(
  formula = paste("Qma1g1", qmodel, sep = "~"),
  data = tmpdat[tmpdat$a == 1, ], family = "quasibinomial"
)
Qzfita0g0 <- glm(
  formula = paste("Qma0g0", qmodel, sep = "~"),
  data = tmpdat[tmpdat$a == 0, ], family = "quasibinomial"
)

Qza1g0 <- predict(Qzfita1g0, type = "response", newdata = tmpdat)
Qza1g1 <- predict(Qzfita1g1, type = "response", newdata = tmpdat)
Qza0g0 <- predict(Qzfita0g0, type = "response", newdata = tmpdat)
```

(Note that if $A$ were not randomly assigned, we would need to complete a
second targeting step.)

10. The empirical mean of these predicted values is the TML estimate of
   $\Psi(P)(a^\prime, \hat{g}_{M \mid a^{\star}, W})$.

```{r, eval=FALSE}
tmlea1m0 <- sum(Qzupa1g0 * svywt) / sum(svywt)
tmlea1m1 <- sum(Qzupa1g1 * svywt) / sum(svywt)
tmlea0m0 <- sum(Qzupa0g0 * svywt) / sum(svywt)
```

11. Repeat the above steps for each of the interventions. For example, for
    binary $A$, we would execute these steps a total of three times to
    estimate:
    1. $\Psi(P)(1,\hat{g}_{M \mid 1, W})$,
    2. $\Psi(P)(1,\hat{g}_{M \mid 0, W})$, and
    3. $\Psi(P)(0,\hat{g}_{M \mid 0, W})$.

12. The PIDE can then be obtained by substituting estimates of parameters
    $\Psi(P)(a,\hat{g}_{M \mid a^{\star}, W}) -
    \Psi(P)(a^{\star},\hat{g}_{M \mid a^{\star}, W})$ and the PIIE
    can be obtained by substituting estimates of parameters
    $\Psi(P)(a,\hat{g}_{M \mid a,W}) -
    \Psi(P)(a, \hat{g}_{M \mid a^{\star}, W})$.

```{r, eval=FALSE}
nde <- tmlea1m0 - tmlea0m0
nie <- tmlea1m1 - tmlea1m0
```

13. The variance can be estimated as the sample variance of the EIF (defined
    above, substituting in the targeted fits) divided by $n$.

```{r, eval=FALSE}
# first get EIF
tmpdat$qyupa1g0 <- plogis(qlogis(tmpdat$qyinit[, 1]) + epsilonma1g0)
tmpdat$qyupa1g1 <- plogis(qlogis(tmpdat$qyinit[, 1]) + epsilonma1g1)
tmpdat$qyupa0g0 <- plogis(qlogis(tmpdat$qyinit[, 1]) + epsilonma0g0)

eic1a1g0 <- tmpdat$ha1gma0 * (tmpdat$y - tmpdat$qyupa1g0)
eic2a1g0 <- psa1 * svywt * (tmpdat$Qma1g0 - Qzupa1g0)
eic3a1g0 <- Qzupa1g0 - tmlea1m0
eica1g0 <- eic1a1g0 + eic2a1g0 + eic3a1g0

eic1a1g1 <- tmpdat$ha1gma1 * (tmpdat$y - tmpdat$qyupa1g1)
eic2a1g1 <- psa1 * svywt * (tmpdat$Qma1g1 - Qzupa1g1)
eic3a1g1 <- Qzupa1g1 - tmlea1m1
eica1g1 <- eic1a1g1 + eic2a1g1 + eic3a1g1

eic1a0g0 <- tmpdat$ha0gma0 * (tmpdat$y - tmpdat$qyupa0g0)
eic2a0g0 <- psa0 * svywt * (tmpdat$Qma0g0 - Qzupa0g0)
eic3a0g0 <- Qzupa0g0 - tmlea0m0
eica0g0 <- eic1a0g0 + eic2a0g0 + eic3a0g0

# estimands
ndeeic <- eica1g0 - eica0g0
vareic <- var(ndeeic) / nrow(tmpdat)

nieeic <- eica1g1 - eica1g0
varnieeic <- var(nieeic) / nrow(tmpdat)
```

## The general case

Actually, we would want to have the fixed parameter with the true, unknown
$g_{M \mid a, W}$ and would like $M$ to be continuous/multi-dimensional.

This is a pain to do by hand, but Nima made an easy-to-use package for all of us
called [medoutcon](https://github.com/nhejazi/medoutcon)! He will go through
this next.
