[["index.html", "Causal Mediation: Modern Methods for Path Analysis A Workshop at SER 2021 Welcome to SER! 0.1 About this workshop 0.2 Workshop schedule 0.3 About the instructors 0.4 Reproduciblity 0.5 Setup instructions", " Causal Mediation: Modern Methods for Path Analysis A Workshop at SER 2021 Iván Díaz, Nima Hejazi, Kara Rudolph updated: May 20, 2021 Welcome to SER! This open source, reproducible vignette accompanies a half-day workshop on modern methods for causal mediation analysis, given at the SER 2021 Meeting on Monday, 24 May 2021. While we encourage use of this bookdown site, for convenience, we have also made these workshop materials available in PDF. Discussion will take place on Slack – first join the workspace here, then the “#ser2021” channel. 0.1 About this workshop Causal mediation analysis can provide a mechanistic understanding of how an exposure impacts an outcome, a central goal in epidemiology and health sciences. However, rapid methodologic developments coupled with few formal courses presents challenges to implementation. Beginning with an overview of classical direct and indirect effects, this workshop will present recent advances that overcome limitations of previous methods, allowing for: (i) continuous exposures, (ii) multiple, non-independent mediators, and (iii) effects identifiable in the presence of intermediate confounders affected by exposure. Emphasis will be placed on flexible, stochastic and interventional direct and indirect effects, highlighting how these may be applied to answer substantive epidemiological questions from real-world studies. Multiply robust, nonparametric estimators of these causal effects, and free and open source R packages (medshift and medoutcon) for their application, will be introduced. To ensure translation to real-world data analysis, this workshop will incorporate hands-on R programming exercises to allow participants practice in implementing the statistical tools presented. It is recommended that participants have working knowledge of the basic notions of causal inference, including counterfactuals and identification (linking the causal effect to a parameter estimable from the observed data distribution). Familiarity with the R programming language is also recommended. 0.2 Workshop schedule 09:00A-09:30A: Introductions + mediation set-up 09:30A-10:15A: Controlled direct effects, natural direct/indirect effects, interventional direct/indirect effects 10:15A-10:45A: Stochastic mediation estimands 10:45A-11:00A: Choosing an estimand in real-world examples 11:00A-11:15A: Break + discussion 11:15A-11:45A: What is the EIF?! 11:45A-12:00P: Using the EIF for estimating the natural direct effect 12:00P-12:45P: Example walkthrough with R packages for effect estimation 12:45P-01:00P: Wrap-up NOTE: All times listed in Pacific Time. 0.3 About the instructors Iván Díaz I am an Assistant Professor at Weill Cornel Medicine. My research focuses on the development of non-parametric statistical methods for causal inference from observational and randomized studies with complex datasets, using machine learning. This includes but is not limited to mediation analysis, methods for continuous exposures, longitudinal data including survival analysis, and efficiency guarantees with covariate adjustment in randomized trials. I am also interested in general semi-parametric theory, machine learning, and high-dimensional data. Nima Hejazi I am a PhD candidate in biostatistics at UC Berkeley, working under the joint direction of Mark van der Laan and Alan Hubbard. My research interests fall at the intersection of causal inference and machine learning, drawing on ideas from non/semi-parametric estimation in large, flexible statistical models. Particular areas of current emphasis include causal mediation analysis, corrections for outcome-dependent sampling designs, targeted loss-based estimation, and applications in vaccine efficacy trials. I am also passionate about statistical computing and open source software development for applied statistics. Kara Rudolph I am an Assistant Professor of Epidemiology at Columbia University. My research interests are in developing and applying causal inference methods to understand social and contextual influences on mental health, substance use, and violence in disadvantaged, urban areas of the United States. My current work focuses on developing methods for transportability and mediation, and subsequently applying those methods to understand how aspects of the school and peer environments mediate relationships between neighborhood factors and adolescent drug use across populations. More generally, my work on generalizing/ transporting findings from study samples to target populations and identifying subpopulations most likely to benefit from interventions contributes to efforts to optimally target available policy and program resources. 0.4 Reproduciblity These workshop materials were written using bookdown, and the complete source is available on GitHub. This version of the book was built with R version 4.0.5 (2021-03-31), pandoc version r rmarkdown::pandoc_version(), and the following packages: package version source bookdown 0.21.11 Github (rstudio/bookdown@33c4f70) bslib 0.2.4.9003 Github (rstudio/bslib@e09af88) data.table 1.14.0 CRAN (R 4.0.5) downlit 0.2.1 CRAN (R 4.0.5) dplyr 1.0.6 CRAN (R 4.0.5) ggfortify 0.4.11 CRAN (R 4.0.5) ggplot2 3.3.3 CRAN (R 4.0.5) kableExtra 1.3.4 CRAN (R 4.0.5) knitr 1.32 CRAN (R 4.0.5) magick 2.7.1 CRAN (R 4.0.5) medoutcon 0.1.5 Github (nhejazi/medoutcon@39820e2) medshift 0.1.4 Github (nhejazi/medshift@f9e11a9) mvtnorm 1.1-1 CRAN (R 4.0.5) origami 1.0.3 CRAN (R 4.0.5) pdftools 2.3.1 CRAN (R 4.0.5) readr 1.4.0 CRAN (R 4.0.5) rmarkdown 2.7.11 Github (rstudio/rmarkdown@e340d75) skimr 2.1.3 CRAN (R 4.0.5) sl3 1.4.3 Github (tlverse/sl3@5cddc6c) stringr 1.4.0 CRAN (R 4.0.5) tibble 3.1.1 CRAN (R 4.0.5) tidyr 1.1.3 CRAN (R 4.0.5) 0.5 Setup instructions 0.5.1 R and RStudio R and RStudio are separate downloads and installations. R is the underlying statistical computing environment. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio. 0.5.1.1 Windows 0.5.1.1.1 If you already have R and RStudio installed Open RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio. To check which version of R you are using, start RStudio and the first thing that appears in the console indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it. You can check here for more information on how to remove old versions from your system if you wish to do so. 0.5.1.1.2 If you don’t have R and RStudio installed Download R from the CRAN website. Run the .exe file that was just downloaded Go to the RStudio download page Under Installers select RStudio x.yy.zzz - Windows XP/Vista/7/8 (where x, y, and z represent version numbers) Double click the file to install it Once it’s installed, open RStudio to make sure it works and you don’t get any error messages. 0.5.1.2 Mac OSX 0.5.1.2.1 If you already have R and RStudio installed Open RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio. To check the version of R you are using, start RStudio and the first thing that appears on the terminal indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it. 0.5.1.2.2 If you don’t have R and RStudio installed Download R from the CRAN website. Select the .pkg file for the latest R version Double click on the downloaded file to install R It is also a good idea to install XQuartz (needed by some packages) Go to the RStudio download page Under Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit) (where x, y, and z represent version numbers) Double click the file to install RStudio Once it’s installed, open RStudio to make sure it works and you don’t get any error messages. 0.5.1.3 Linux Follow the instructions for your distribution from CRAN, they provide information to get the most recent version of R for common distributions. For most distributions, you could use your package manager (e.g., for Debian/Ubuntu run sudo apt-get install r-base, and for Fedora sudo yum install R), but we don’t recommend this approach as the versions provided by this are usually out of date. In any case, make sure you have at least R 3.3.1. Go to the RStudio download page Under Installers select the version that matches your distribution, and install it with your preferred method (e.g., with Debian/Ubuntu sudo dpkg -i rstudio-x.yy.zzz-amd64.deb at the terminal). Once it’s installed, open RStudio to make sure it works and you don’t get any error messages. These setup instructions are adapted from those written for Data Carpentry: R for Data Analysis and Visualization of Ecological Data. 0.5.2 Virtual Enironment setup with renv These instructions are intended to help with setting up the included renv virtual environment, which ensures all participants are using the same exact set of R packages (and package versions). A few important notes to keep in mind: When R is started from the top level of this repository, renv is activated automatically. There is no further action required on your part. If renv is not installed, it will be installed automatically, assuming that you have an active internet connection. While renv is active, the R session will only have access to the packages (and their dependencies) that are listed in the renv.lock file – that is, you should not expect to have access to any other R packages that may be installed elsewhere on the computing system in use. Upon an initial attempt, renv will prompt you to install packages listed in the renv.lock file, by printing a message like the following: # * Project &#39;PATH/TO/ser2021_mediation_workshop&#39; loaded. [renv 0.13.2] # * The project may be out of sync -- use `renv::status()` for more details. &gt; renv::status() # The following package(s) are recorded in the lockfile, but not installed: # Use `renv::restore()` to install these packages. In any such case, please call renv::restore() to install any missing packages. Note that you do not need to manually install the packages via install.packages(), remotes::install_github(), or similar. For details on how the renv system works, the following references may be helpful: Collaborating with renv Introduction to renv In some rare cases, R packages that renv automatically tries to install as part of the renv::restore() process may fail due to missing systems-level dependencies. In such cases, a reference to the missing dependencies and system-specific instructions their installation involving, e.g., Ubuntu Linux’s apt or homebrew for macOS, will usually be displayed. "],["mediation.html", "Chapter 1 Causal mediation analysis intro 1.1 Motivating study 1.2 What is causal mediation analysis? 1.3 Causal mediation models 1.4 Counterfactuals", " Chapter 1 Causal mediation analysis intro 1.1 Motivating study A recent, large, multi-site trial (X:BOT) compared the effectiveness of XR-NTX to buprenorphine–naloxone (BUP-NX) in preventing relapse among those with OUD starting medication in inpatient treatment settings. An analysis of potential moderators of medication effectiveness found that homeless individuals had a lower risk of relapse on XRNTX, whereas non-homeless individuals had a lower risk of relapse on BUP-NX. The effect sizes were similarly large for these groups but in opposite directions. The underlying mechanisms for these differences have not yet been explored. We can to identify these mechanisms by using mediation analysis. Key questions: Do differences in the effects of treatment (comparing two medications for opioid use disorder, naltrexone vs buprenorphine) on risk of relapse operate through mediators of adherence, opioid use, pain, and depressive symptoms? (Rudolph et al. 2020) Are those mediated effects different for homeless vs non-homeless individuals? 1.2 What is causal mediation analysis? Statistical mediation analyses assess associations between the variables Causal mediation analyses assess how the paths behave under circumstances different from the observed circumstances (e.g., interventions) 1.2.1 Why are the methods that we will discuss today important? Assume you are interested in the effect of treatment assignment \\(A\\) (naltrexone vs. buprenorphine) on an outcome \\(Y\\) (risk of relapse) through mediators \\(M\\) (opioid use, pain, depressive symptoms) We have pre-treatment confounders \\(W\\) There is a confounder \\(Z\\) of \\(M\\rightarrow Y\\) ffected by treatment assignment (adherence) We could fit the following models: \\[\\begin{align} \\E(M\\mid A=a, W=w, Z=z) &amp; = \\gamma_0 + \\gamma_1 a + \\gamma_2 w + \\gamma_3 z \\\\ \\E(Y\\mid M=m, A=a, W=w, Z=z) &amp; = \\beta_0 + \\beta_1 m + \\beta_2 a + \\beta_3 w + \\beta_4 z \\end{align}\\] The product \\((\\gamma_1\\beta_1)\\) has been proposed as a measure of the effect of \\(A\\) on \\(Y\\) through \\(M\\) Causal interpretation problems with this method: We will see that this parameter cannot be interpreted as a causal effect 1.2.2 R Example: Assume we have a pre-treamtment confounder of \\(Y\\) and \\(M\\), denote it with \\(W\\) For simplicity, assume \\(A\\) is randomized We’ll generate a really large sample from a data generating mechanism so that we are not concerned with sampling errors n &lt;- 1e6 w &lt;- rnorm(n) a &lt;- rbinom(n, 1, 0.5) z &lt;- rbinom(n, 1, 0.2 * a + 0.3) m &lt;- rnorm(n, w + z) y &lt;- rnorm(n, m + w - a + z) Note that the indirect effect (i.e., the effect through \\(M\\)) in this example is nonzero (there is a pathway \\(A\\rightarrow Z \\rightarrow M \\rightarrow Y\\)) Let’s see what the product of coefficients method would say: lm_y &lt;- lm(y ~ m + a + w + z) lm_m &lt;- lm(m ~ a + w + z) ## product of coefficients coef(lm_y)[2] * coef(lm_m)[2] #&gt; m #&gt; -0.0014835 Among other things, in this workshop: We will provide some understanding for why the above method fails in this example We will study estimators that are robust to misspecification in the above models 1.3 Causal mediation models In this workshop we will use directed acyclic graphs. We will focus on the two types of graph: 1.3.1 No intermediate confounders FIGURE 1.1: Directed acyclic graph under no intermediate confounders of the mediator-outcome relation affected by treatment 1.3.2 Intermediate confounders FIGURE 1.2: Directed acyclic graph under intermediate confounders of the mediator-outcome relation affected by treatment The above graphs can be interpreted as a non-parametric structural equation model (NPSEM), also known as structural causal model (SCM): \\[\\begin{align} W &amp; = f_W(U_W)\\\\ A &amp; = f_A(W, U_A)\\\\ Z &amp; = f_Z(W, A, U_Z)\\\\ M &amp; = f_M(W, A, Z, U_M)\\\\ Y &amp; = f_Y(W, A, Z, M, U_Y) \\end{align}\\] Here \\(U=(U_W, U_A, U_Z, U_M, U_Y)\\) is a vector of all unmeasured exogenous factors affecting the system The functions \\(f\\) are assumed fixed but unknown We posit this model as a system of equations that nature uses to generate the data Therefore we leave the functions \\(f\\) unspecified (i.e., we do not know the true nature mechanisms) Sometimes we know something: e.g., if \\(A\\) is randomized we know \\(A=f_A(U_A)\\) where \\(U_A\\) is the flip of a coin (i.e., independent of everything). 1.4 Counterfactuals Recall that we are interested in assessing how the pathways would behave under circumstances different from the observed circumstances We operationalize this idea using counterfactual variables Counterfactuals are hypothetical random variables that would have been observed in an alternative world where something had happened, possibly contrary to fact We will constantly use the following counterfactual variables: \\(Y_a\\) is a counterfactual variable in a hypothetical world where \\(\\P(A=a)=1\\) with probability one for some valure \\(a\\) \\(Y_{a,m}\\) is the counterfactual outcome in a world where \\(\\P(A=a,M=m)=1\\) \\(M_a\\) is the counterfactual variable representing the mediator in a world where \\(\\P(A=a)=1\\). 1.4.1 How are counterfactuals defined? In the NPSEM framework, counterfactuals are quantities derived from the model. Once you define a change to the causal system, that change needs to be progragated downstream. Example: modifying the system to make everyone receive XR-NTX yields counterfactual adherence, mediators, and outcomes. Take as example the DAG in Figure 1.2: \\[\\begin{align} A &amp;= a\\\\ Z_a &amp;= f_Z(W, a, U_M)\\\\ M_a &amp;= f_M(W, a, Z_a, U_M)\\\\ Y_a &amp;= f_Y(W, a, Z_a, M_a, U_Y)\\\\ \\end{align}\\] We will also be interested in _joint changes to the system: \\[\\begin{align} A &amp;= a\\\\ Z_a &amp;= f_Z(W, a, U_M)\\\\ M &amp;= m\\\\ Y_{a,m} &amp;= f_Y(W, a, Z_a, m, U_Y)\\\\ \\end{align}\\] And, perhaps more importantly, we will use nested counterfactuals For example, if \\(A\\) is binary, you can think of the following counterfactual \\[\\begin{align} A &amp;= 1\\\\ Z_1 &amp;= f_Z(W, 1, U_M)\\\\ M &amp;= M_0\\\\ Y_{1, M_0} &amp;= f_Y(W, 1, Z_1, M_0, U_Y)\\\\ \\end{align}\\] \\(Y_{1, M_0}\\) is interpreted as the outcome for an individual in a hypothetical world where treatment was given but the mediator was held at the value it would have taken under no treatment Causal mediation effects are often defined in terms of the distribution of these nested counterfactuals. That is, causal effects give you information about what would have happened in some hypothetical world where the mediator and treatment mechanisms changed. "],["estimands.html", "Chapter 2 Types of path-specific causal mediation effects 2.1 Controlled direct effects 2.2 Natural direct and indirect effects 2.3 Interventional (in)direct effects 2.4 Estimand Summary", " Chapter 2 Types of path-specific causal mediation effects Controlled direct effects Natural direct and indirect effects Interventional direct and indirect effects FIGURE 2.1: Directed acyclic graph under no intermediate confounders of the mediator-outcome relation affected by treatment 2.1 Controlled direct effects \\[\\psi_{\\text{CDE}} = \\E(Y_{1,m} - Y_{0,m}) \\] Set the mediator to a reference value \\(M=m\\) uniformly for everyone in the population Compare \\(A=1\\) vs \\(A=0\\) with \\(M=m\\) fixed 2.1.1 Identification assumptions: Confounder assumptions: \\(A \\indep Y_{a,m} \\mid W\\) \\(M \\indep Y_{a,m} \\mid W, A\\) Positivity assumptions: \\(\\P(M = m \\mid A=a, W) &gt; 0 \\text{ } a.e.\\) \\(\\P(A=a \\mid W) &gt; 0 \\text{ } a.e.\\) Under the above identification assumptions, the controlled direct effect can be identified: \\[ \\E(Y_{1,m} - Y_{0,m}) = \\E\\{\\color{ForestGreen}{\\E(Y \\mid A=1, M=m, W) - \\E(Y \\mid A=0, M=m, W)}\\}\\] For intuition about this formula in R, let’s continue with a toy example: n &lt;- 1e6 w &lt;- rnorm(n) a &lt;- rbinom(n, 1, 0.5) m &lt;- rnorm(n, w + a) y &lt;- rnorm(n, w + a + m) First we fit a correct model for the outcome lm_y &lt;- lm(y ~ m + a + w) Assume we would like the CDE at \\(m=0\\) Then we generate predictions \\[\\color{ForestGreen}{\\E(Y \\mid A=1, M=m, W)} \\text{ and }\\color{ForestGreen}{\\E(Y \\mid A=0, M=m, W)}:\\] pred_y1 &lt;- predict(lm_y, newdata = data.frame(a = 1, m = 0, w = w)) pred_y0 &lt;- predict(lm_y, newdata = data.frame(a = 0, m = 0, w = w)) Then we compute the difference between the predicted values \\(\\color{ForestGreen}{\\E(Y \\mid A=1, M=m, W) - \\E(Y \\mid A=0, M=m, W)}\\), and average across values of \\(W\\) ## CDE at m = 0 mean(pred_y1 - pred_y0) #&gt; [1] 1.0009 2.1.2 Is this the estimand I want? Makes the most sense if can intervene directly on \\(M\\) And can think of a policy that would set everyone to a single constant level \\(m \\in \\mathcal{M}\\). J. Pearl calls this prescriptive. Can you think of an example? Air pollution, rescue inhaler dosage, hospital visits Does not provide a decomposition of the average treatment effect into direct and indirect effects What if our research question doesn’t involve intervening directly on the mediator? What if we want to decompose the average treatment effect into its direct and indirect counterparts? 2.2 Natural direct and indirect effects Still using the same DAG as above, Recall the definition of the nested counterfactual \\[\\begin{equation*} Y_{1, M_0} = f_Y(W, 1, M_0, U_Y) \\end{equation*}\\] Interpreted as the outcome for an individual in a hypothetical world where treatment was given but the mediator was held at the value it would have taken under no treatment Recall that, because of the definition of counterfactuals \\[\\begin{equation*} Y_{1, M_1} = Y_1 \\end{equation*}\\] Then we can decompose the average treatment effect \\(E(Y_1-Y_0)\\) as follows \\[\\begin{equation*} \\E[Y_{1,M_1} - Y_{0,M_0}] = \\underbrace{\\E[Y_{\\color{red}{1},\\color{blue}{M_1}} - Y_{\\color{red}{1},\\color{blue}{M_0}}]}_{\\text{natural indirect effect}} + \\underbrace{\\E[Y_{\\color{blue}{1},\\color{red}{M_0}} - Y_{\\color{blue}{0},\\color{red}{M_0}}]}_{\\text{natural direct effect}} \\end{equation*}\\] Natural direct effect (NDE): Varying treatment while keeping the mediator fixed at the value it would have taken under no treatment Natural indirect effect (NIE): Varying the mediator from the value it would have taken under treatment to the value it would have taken under control, while keeping treatment fixed 2.2.1 Identification assumptions: \\(A \\indep Y_{a,m} \\mid W\\) \\(M \\indep Y_{a,m} \\mid W, A\\) \\(A \\indep M_a \\mid W\\) \\(M_0 \\indep Y_{1,m} \\mid W\\) and positivity assumptions 2.2.2 Cross-world independence assumption What does \\(M_0 \\indep Y_{1,m} \\mid W\\) mean? Conditional on \\(W\\), knowledge of the mediator value in the absence of treatment, \\(M_0\\), provides no information about the outcome under treatment, \\(Y_{1,m}\\). Can you think of a data-generating mechanism that would violate this assumption? Example: in a randomized study, whenever we believe that treatment assignment works through adherence (i.e., almost always), we are violating this assumption (more on this later). Cross-world assumptions are problematic for other reasons, including: You can never design a randomized study where the assumption holds by design. If the cross-world assumption holds, can write the NDE as a weighted average of controlled direct effects at each level of \\(M=m\\). \\[\\E \\sum_m \\{\\E(Y_{1,m} \\mid W) - \\E(Y_{0,m} \\mid W)\\} \\P(M_{0}=m \\mid W)\\] If CDE(\\(m\\)) is constant across \\(m\\), then CDE = NDE. 2.2.3 Identification formula: Under the above identification assumptions, the natural direct effect can be identified: \\[\\begin{equation*} \\E(Y_{1,M_0} - Y_{0,M_0}) = \\E[\\color{Goldenrod}{\\E\\{}\\color{ForestGreen}{\\E(Y \\mid A=1, M, W) - \\E(Y \\mid A=0, M, W)}\\color{Goldenrod}{\\mid A=0,W\\}}] \\end{equation*}\\] The natural indirect effect can be identified similarly. Let’s dissect this formula in R: n &lt;- 1e6 w &lt;- rnorm(n) a &lt;- rbinom(n, 1, 0.5) m &lt;- rnorm(n, w + a) y &lt;- rnorm(n, w + a + m) First we fit a correct model for the outcome lm_y &lt;- lm(y ~ m + a + w) Then we generate predictions \\[\\color{ForestGreen}{\\E(Y \\mid A=1, M, W)} \\text{ and }\\color{ForestGreen}{\\E(Y \\mid A=0, M, W)}\\] with \\(A\\) fixed but letting \\(M\\) and \\(W\\) take their observed values pred_y1 &lt;- predict(lm_y, newdata = data.frame(a = 1, m = m, w = w)) pred_y0 &lt;- predict(lm_y, newdata = data.frame(a = 0, m = m, w = w)) Then we compute the difference between the predicted values \\[\\color{ForestGreen}{\\E(Y \\mid A=1, M, W) - \\E(Y \\mid A=0, M, W)},\\] and use this difference as a pseudo-outcome in a regression on \\(A\\) and \\(W\\): \\[\\color{Goldenrod}{\\E\\{}\\color{ForestGreen}{\\E(Y \\mid A=1, M, W) - \\E(Y \\mid A=0, M, W)}\\color{Goldenrod}{\\mid A=0,W\\}}\\] pseudo &lt;- pred_y1 - pred_y0 lm_pseudo &lt;- lm(pseudo ~ a + w) Now we predict the value of this pseudo-outcome under \\(A=0\\), and average the result pred_pseudo &lt;- predict(lm_pseudo, newdata = data.frame(a = 0, w = w)) ## NDE: mean(pred_pseudo) #&gt; [1] 0.99655 2.2.4 Is this the estimand I want? Makes sense to intervene on \\(A\\) but not directly on \\(M\\). Want to understand a natural mechanism underlying an association/ total effect. J. Pearl calls this descriptive. NDE + NIE = total effect (ATE). Okay with the assumptions. What if our data structure involves a post-treatment confounder of the mediator-outcome relationship (e.g., adherence)? FIGURE 2.2: Directed acyclic graph under intermediate confounders of the mediator-outcome relation affected by treatment 2.2.5 Unidentifiability of the NDE and NIE in this setting In this example, natural direct and indirect effects are unidentifiable from observed data \\(O=(W,A,Z,M,Y)\\). The reason for this is that the cross-world counterfactual assumption \\[\\begin{equation*} Y_{1,m}\\indep M_0\\mid W \\end{equation*}\\] does not hold in the above directed acyclic graph. Technically, the reason for this is that an intervention setting \\(A=1\\) (necessary for the definition of \\(Y_{1,m}\\)) induces a counterfactual variable \\(Z_1\\). Likewise, an intervention setting \\(A=0\\) (necessary for the definition of \\(M_0\\)) induces a counterfactual \\(Z_0\\). The variables \\(Z_1\\) and \\(Z_0\\) are correlated because they share unmeasured common causes, \\(U_Z\\). The variable \\(Z_1\\) is correlated with \\(Y_{1,m}\\), and the variable \\(Z_0\\) is correlated with \\(M_0\\), because they are counterfactual outcomes in the same hypothetical worlds. To see this in the definition of counterfactual from a causal structural model: \\[\\begin{align*} Y_{1,m} &amp;= f_Y(W, 1, Z_1, m, U_Y), \\text{ and }\\\\ M_0 &amp;= f_M(W, 0, Z_0, U_M)\\\\ \\end{align*}\\] are correlated even after adjusting for \\(W\\) by virtue of \\(Z_1\\) and \\(Z_0\\) being correlated. Note: CDEs are still identified in this setting. They can be identified and estimated similarly to a longitudinal data sructure with a two-time-point intervention. 2.3 Interventional (in)direct effects Let \\(G_a\\) denote a random draw from the distribution of \\(M_a \\mid W\\) Define the counterfactual \\(Y_{1,G_0}\\) as the counterfactual variable in a hypothetical world where \\(A\\) is set \\(A=1\\) and \\(M\\) is set to \\(M=G_0\\) with probability one. Define \\(Y_{0,G_0}\\) and \\(Y_{1,G_1}\\) similarly Then we can define: \\[\\begin{equation*} \\E[Y_{1,G_1} - Y_{0,G_0}] = \\underbrace{\\E[Y_{\\color{red}{1},\\color{blue}{G_1}} - Y_{\\color{red}{1},\\color{blue}{G_0}}]}_{\\text{interventional indirect effect}} + \\underbrace{\\E[Y_{\\color{blue}{1},\\color{red}{G_0}} - Y_{\\color{blue}{0},\\color{red}{G_0}}]}_{\\text{interventional direct effect}} \\end{equation*}\\] Note that \\(\\E[Y_{1,G_1} - Y_{0,G_0}]\\) is still a total effect of treatment, even if it is different from the ATE \\(\\E[Y_{1} - Y_{0}]\\) We gain in the ability to solve a problem, but lose in terms of interpretation of the causal effect (cannot decompose the ATE) 2.3.1 An alternative definition of the effects: Above we defined \\(G_a\\) as a random draw from the distribution of \\(M_a \\mid W\\) What if instead we define \\(G_a\\) as a random draw from the distribution of \\(M_a \\mid (Z_a,W)\\) It turns out the indirect effect defined in this way only measures the path \\(A\\rightarrow M \\rightarrow Y\\), and not the path \\(A\\rightarrow Z\\rightarrow M \\rightarrow Y\\) There may be important reasons to choose one over another (e.g., survival analyses where we want the distribution conditional on \\(Z\\), instrumental variable designs where it doesn’t make sense to condition on \\(Z\\)) 2.3.2 Identification assumptions: \\(A \\indep Y_{a,m} \\mid W\\) \\(M \\indep Y_{a,m} \\mid W, A, Z\\) \\(A \\indep M_a \\mid W\\) and positivity assumptions. Under these assumptions, the population interventional direct and indirect effect is identified: \\[\\begin{align*} \\E&amp;(Y_{a, G_{a&#39;}}) = \\\\ &amp;\\E\\left[\\color{Purple}{\\E\\left\\{\\color{Goldenrod}{\\sum_z} \\color{ForestGreen}{\\E(Y \\mid A=a, Z=z, M, W)} \\color{Goldenrod}{\\P(Z=z \\mid A=a, W)}\\mid A=a&#39;, W\\right\\}}\\right] \\end{align*}\\] Let’s dissect this formula in R: n &lt;- 1e6 w &lt;- rnorm(n) a &lt;- rbinom(n, 1, 0.5) z &lt;- rbinom(n, 1, 0.5 + 0.2 * a) m &lt;- rnorm(n, w + a - z) y &lt;- rnorm(n, w + a + z + m) Let us compute \\(\\E(Y_{1, G_0})\\) (so that \\(a = 1\\), and \\(a&#39;=0\\)). First, fit a regression model for the outcome, and compute \\[\\color{ForestGreen}{\\E(Y \\mid A=a, Z=z, M, W)}\\] for all values of \\(z\\) lm_y &lt;- lm(y ~ m + a + z + w) pred_a1z0 &lt;- predict(lm_y, newdata = data.frame(m = m, a = 1, z = 0, w = w)) pred_a1z1 &lt;- predict(lm_y, newdata = data.frame(m = m, a = 1, z = 1, w = w)) Now we fit the true model for \\(Z \\mid A, W\\) and get the conditional probability that \\(Z=1\\) fixing \\(A=1\\) prob_z &lt;- lm(z ~ a) pred_z &lt;- predict(prob_z, newdata = data.frame(a = 1)) Now we compute the following pseudo-outcome: \\[\\color{Goldenrod}{\\sum_z}\\color{ForestGreen}{\\E(Y \\mid A=a, Z=z, M, W)} \\color{Goldenrod}{\\P(Z=z \\mid A=a, w)}\\] pseudo_out &lt;- pred_a1z0 * (1 - pred_z) + pred_a1z1 * pred_z Now we regress this pseudo-outcome on \\(A,W\\), and compute the predictions setting \\(A=0\\), that is, \\[\\color{Purple}{\\E\\left\\{\\color{Goldenrod}{\\sum_z} \\color{ForestGreen}{\\E(Y \\mid A=a, Z=z, M, W)} \\color{Goldenrod}{\\P(Z=z \\mid A=a, w)}\\mid A=a&#39;, W\\right\\}}\\] fit_pseudo &lt;- lm(pseudo_out ~ a + w) pred_pseudo &lt;- predict(fit_pseudo, data.frame(a = 0, w = w)) And finally, just average those predictions! ## Mean(Y(1, G(0))) mean(pred_pseudo) #&gt; [1] 1.1979 This was for \\((a,a&#39;)=(1,0)\\). Can do the same with \\((a,a&#39;)=(1,1)\\), and \\((a,a&#39;)=(0,0)\\) to obtain an effect decomposition \\[\\begin{equation*} \\E[Y_{1,G_1} - Y_{0,G_0}] = \\underbrace{\\E[Y_{\\color{red}{1}, \\color{blue}{G_1}} - Y_{\\color{red}{1}, \\color{blue}{G_0}}]}_{\\text{interventional indirect effect}} + \\underbrace{\\E[Y_{\\color{blue}{1},\\color{red}{G_0}} - Y_{\\color{blue}{0}, \\color{red}{G_0}}]}_{\\text{interventional direct effect}} \\end{equation*}\\] 2.3.3 Is this the estimand I want? Makes sense to intervene on \\(A\\) but not directly on \\(M\\). Goal is to understand a natural mechanism underlying an association or total effect. Okay with the assumptions! 2.4 Estimand Summary FIGURE 2.3: Excerpted from Rudolph et al. (2019) "],["stochastic.html", "Chapter 3 Stochastic direct and indirect effects 3.1 Definition of the effects 3.2 Motivation for stochastic interventions 3.3 Definition of stochastic effects 3.4 Identification assumptions 3.5 What are the odds of exposure under intervention vs real world? 3.6 Summary", " Chapter 3 Stochastic direct and indirect effects 3.1 Definition of the effects Consider the following directed acyclic graph. FIGURE 2.1: Directed acyclic graph under no intermediate confounders of the mediator-outcome relation affected by treatment 3.2 Motivation for stochastic interventions So far we have discussed controlled, natural, and interventional (in)direct effects These effects require that \\(0 &lt; \\P(A=1\\mid W) &lt; 1\\) They are defined only for binary exposures What can we do when the positivity assumption does not hold or the exposure is continuous? Solution: we can use stochastic effects 3.3 Definition of stochastic effects There are two possible ways of defining stochastic effects: Consider the effect of an intervention where the exposure is drawn from a distribution For example incremental propensity score interventions Consider the effect of an intervention where the post-intervention exposure is a function of the actually received exposure For example modified treatment policies In both cases \\(A \\mid W\\) is a non-deterministic intervention, thus the name stochastic intervention 3.3.1 Example: incremental propensity score interventions (IPSI) (Kennedy 2018) Definition of the intervention Assume \\(A\\) is binary, and \\(\\P(A=1\\mid W=w) = g(1\\mid w)\\) is the propensity score Consider an intervention in which each individual receives the intervention with probability \\(g_\\delta(1\\mid w)\\), equal to \\[\\begin{equation*} g_\\delta(1\\mid w)=\\frac{\\delta g(1\\mid w)}{\\delta g(1\\mid w) + 1 - g(1\\mid w)} \\end{equation*}\\] e.g., draw the post-intervention exposure from a Bernoulli variable with probability \\(g_\\delta(1\\mid w)\\) The value \\(\\delta\\) is user given Let \\(A_\\delta\\) denote the post-intervention exposure distribution Some algebra shows that \\(\\delta\\) is an odds ratio comparing the pre- and post-intervention exposure distributions \\[\\begin{equation*} \\delta = \\frac{\\text{odds}(A_\\delta = 1\\mid W=w)} {\\text{odds}(A = 1\\mid W=w)} \\end{equation*}\\] Interpretation: what would happen in a world where the odds of receiving treatment is increased by \\(\\delta\\) Let \\(Y_{A_\\delta}\\) denote the outcome in this hypothetical world 3.3.1.1 Illustrative application for IPSIs Consider the effect of participation in sports on children’s BMI Mediation through snacking, exercising, etc. Intervention: for each individual, increase the odds of participating in sports by \\(\\delta=2\\) The post-intervention exposure is a draw \\(A_\\delta\\) from a Bernoulli distribution with probability \\(g_\\delta(1\\mid w)\\) Example: modified treatment policies (MTP) (Dı́az and Hejazi 2020) Definition of the intervention Consider a continuous exposure \\(A\\) taking values in the real numbers Consider an intervention that assigns exposure as \\(A_\\delta = A - \\delta\\) Example: \\(A\\) is pollution measured as \\(PM_{2.5}\\) and you are interested in an intervention that reduces \\(PM_{2.5}\\) concentration by some amount \\(\\delta\\) 3.3.2 Mediation analysis for stochastic interventions The total effect of an IPSI can be computed as a contrast of the outcome under intervention vs no intervention: \\[\\begin{equation*} \\psi = \\E[Y_{A_\\delta} - Y] \\end{equation*}\\] Recall the NPSEM \\[\\begin{align} W &amp; = f_W(U_W)\\\\ A &amp; = f_A(W, U_A)\\\\ M &amp; = f_M(W, A, U_M)\\\\ Y &amp; = f_Y(W, A, M, U_Y) \\end{align}\\] From this we have \\[\\begin{align*} M_{A_\\delta} &amp; = f_M(W, A_\\delta, U_M)\\\\ Y_{A_\\delta} &amp; = f_Y(W, A_\\delta, M_{A_\\delta}, U_Y) \\end{align*}\\] Thus, we have \\(Y_{A_\\delta} = Y_{A_\\delta, M_{A_\\delta}}\\) and \\(Y = Y_{A,M_{A}}\\) Let us introduce the counterfactual \\(Y_{A_\\delta, M}\\), interpreted as the outcome observed in a world where the intervention on \\(A\\) is performed but the mediator is fixed at the value it would have taken under no intervention: \\[Y_{A_\\delta, M} = f_Y(W, A_\\delta, M_{A_\\delta}, U_Y)\\] Then we can decompose the total effect into: \\[\\begin{align*} \\E[Y&amp;_{A_\\delta,M_{A_\\delta}} - Y_{A,M_A}] = \\\\ &amp;\\underbrace{\\E[Y_{\\color{red}{A_\\delta},\\color{blue}{M_{A_\\delta}}} - Y_{\\color{red}{A_\\delta},\\color{blue}{M}}]}_{\\text{stochastic natural indirect effect}} + \\underbrace{\\E[Y_{\\color{blue}{A_\\delta},\\color{red}{M}} - Y_{\\color{blue}{A},\\color{red}{M}}]}_{\\text{stochastic natural direct effect}} \\end{align*}\\] 3.4 Identification assumptions Confounder assumptions: \\(A \\indep Y_{a,m} \\mid W\\) \\(M \\indep Y_{a,m} \\mid W, A\\) No confounder of \\(M\\rightarrow Y\\) affected by \\(A\\) Positivity assumptions: If \\(g_\\delta(a \\mid w)&gt;0\\) then \\(g(a \\mid w)&gt;0\\) If \\(\\P(Z=z\\mid W=w)&gt;0\\) then \\(\\P(Z=z\\mid A=a,W=w)&gt;0\\) Under these assumptions, stochastic effects are identified as follows The indirect effect can be identified as follows \\[\\begin{align*} \\E&amp;(Y_{A_\\delta} - Y_{A_\\delta, M}) =\\\\ &amp;\\E\\left[\\color{Goldenrod}{\\sum_{a}\\color{ForestGreen}{\\{\\E(Y\\mid A=a, W)-\\E(Y\\mid A=a, M, W)\\}}g_\\delta(a\\mid W)}\\right] \\end{align*}\\] The direct effect can be identified as follows \\[\\begin{align*} \\E&amp;(Y_{A_\\delta} - Y_{A_\\delta, M}) =\\\\ &amp;\\E\\left[\\color{Goldenrod}{\\sum_{a}\\color{ForestGreen}{\\{\\E(Y\\mid A=a, M, W) - Y\\}}g_\\delta(a\\mid W)}\\right] \\end{align*}\\] Let’s dissect the formula for the indirect effect in R: n &lt;- 1e6 w &lt;- rnorm(n) a &lt;- rbinom(n, 1, plogis(1 + w)) m &lt;- rnorm(n, w + a) y &lt;- rnorm(n, w + a + m) First, fit regressions of the outcome on \\((A,W)\\) and \\((M,A,W)\\): fit_y1 &lt;- lm(y ~ m + a + w) fit_y2 &lt;- lm(y ~ a + w) Get predictions fixing \\(A=a\\) for all possible values \\(a\\) pred_y1_a1 &lt;- predict(fit_y1, newdata = data.frame(a = 1, m, w)) pred_y1_a0 &lt;- predict(fit_y1, newdata = data.frame(a = 0, m, w)) pred_y2_a1 &lt;- predict(fit_y2, newdata = data.frame(a = 1, w)) pred_y2_a0 &lt;- predict(fit_y2, newdata = data.frame(a = 0, w)) Compute \\[\\color{ForestGreen}{\\{\\E(Y\\mid A=a, W)-\\E(Y\\mid A=a, M, W)\\}}\\] for each value \\(a\\) pseudo_a1 &lt;- pred_y2_a1 - pred_y1_a1 pseudo_a0 &lt;- pred_y2_a0 - pred_y1_a0 Estimate the propensity score \\(g(1\\mid w)\\) and evaluate the post-intervention propensity score \\(g_\\delta(1\\mid w)\\) pscore_fit &lt;- glm(a ~ w, family = binomial()) pscore &lt;- predict(pscore_fit, type = &#39;response&#39;) ## How do the intervention vs observed propensity score compare pscore_delta &lt;- 2 * pscore / (2 * pscore + 1 - pscore) What do the post-intervention propensity scores look like? plot(pscore, pscore_delta, xlab = &#39;Observed prop. score&#39;, ylab = &#39;Prop. score under intervention&#39;) abline(0, 1) 3.5 What are the odds of exposure under intervention vs real world? odds &lt;- (pscore_delta / (1 - pscore_delta)) / (pscore / (1 - pscore)) summary(odds) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2 2 2 2 2 2 Compute the sum \\[\\color{Goldenrod}{\\sum_{a}\\color{ForestGreen}{\\{\\E(Y\\mid A=a, W)-\\E(Y\\mid A=a, M, W)\\}}g_\\delta(a\\mid W)}\\] indirect &lt;- pseudo_a1 * pscore_delta + pseudo_a0 * (1 - pscore_delta) The average of this value is the indirect effect ## E[Y(Adelta) - Y(Adelta, M)] mean(indirect) #&gt; [1] 0.1091 The direct effect is \\[\\begin{align*} \\E&amp;(Y_{A_\\delta} - Y_{A_\\delta, M}) =\\\\ &amp;\\E\\left[\\color{Goldenrod}{\\sum_{a}\\color{ForestGreen}{\\{\\E(Y\\mid A=a, M, W) - Y\\}}g_\\delta(a\\mid W)}\\right] \\end{align*}\\] Which can be computed as direct &lt;- (pred_y1_a1 - y) * pscore_delta + (pred_y1_a0 - y) * (1 - pscore_delta) mean(direct) #&gt; [1] 0.10934 3.6 Summary Stochastic (in)direct effects Relax the positivity assumption Can be defined for non-binary exposures Do not require a cross-world assumption Still require the absence of intermediate confounders But, compared to the NDE and NIE, we can design a randomized study where identifiability assumptions hold, at least in principle There is a version of these effects that can accomodate intermediate confounders (Hejazi et al. 2020) R implementation to be released soon…stay tuned! "],["estimandirl.html", "Chapter 4 How to choose an estimand: Real-world example 4.1 Comparative effectivness of two medications for opioid use disorder (OUD)", " Chapter 4 How to choose an estimand: Real-world example 4.1 Comparative effectivness of two medications for opioid use disorder (OUD) Motivation: Opposite overall treatment effects for homeless versus nonhomeless participants. This application was explored in detail by Rudolph et al. (2020). 4.1.1 Getting specific about the question To what extent does the indirect effect through mediators of adherence, pain, and depressive symptoms explain the differences in treatment effects on OUD relapse for homeless and nonhomeless individuals? What estimand do we want? Can we set \\(M=m\\) (i.e., same value) for everyone? Are we interested in estimating indirect effects? \\(\\rightarrow\\) So, not controlled direct effect. Do we have an intermediate confounder? Yes, and it’s important. \\(\\rightarrow\\) So, not natural (in)direct effects. So, we’re left with the interventional direct and indirect effects. Do we want to estimate the path through treatment initiation (\\(Z\\))? Yes, so, not the conditional versions of these effects. Estimands: Direct effect: \\(\\E(Y_{1,G_0} - Y_{0,G_0})\\) Indirect effect: \\(\\E(Y_{1,G_1} - Y_{1,G_0})\\) Here \\(G_a\\) is a draw from the distribution of \\(M_a\\mid W\\). Need to incorporate multiple and continuous mediators What if the positivity assumption \\(\\P(A=a\\mid W)&gt;0\\) violated? \\(\\rightarrow\\) Can’t identify or estimate any of the above effects But we can estimate the effect of some stochastic interventions, e.g., IPSIs Tradeoff between feasibility and interpretation What if the exposure variable is continuous? \\(\\rightarrow\\) All the above effects are defined for binary exposures But we can estimate the effect of some stochastic interventions Work in progress (including upcoming R software) "],["preliminaries-on-semiparametric-estimation.html", "Chapter 5 Preliminaries on semiparametric estimation 5.1 From causal to statistical quantities 5.2 Semiparametric estimation", " Chapter 5 Preliminaries on semiparametric estimation 5.1 From causal to statistical quantities We have arrived at identification formulas that express quantities that we care about in terms of observable quantities That is, these formulas express what would have happened in hypothetical worlds in terms of quantities observable in this world This required causal assumptions Many of these assumptions are empirically unverifiable We saw an example where we could relax the cross-world assumption, at the cost of changing the parameter interpretation and where we could relax the positivity assumption, also at the cost of changing the parameter interpretation We are now ready to tackle the estimation problem, i.e., how do we best learn the value of quantities that are observable? The resulting estimation problem can be tackled using statistical assumptions of various degrees of strength Most of these assumptions are verifiable (e.g., a linear model) Thus, most are unnecessary (except for convenience) We have worked hard to try to satisfy the required causal assumptions This is not the time to introcuce unnecessary statistical assumptions The estimation approach we will introduce reduces reliance on these statistical assumptions 5.1.1 Computing identification formulas if you know the true distribution The mediation parameters that we consider can be seen as a function of the joint probability distribution of \\(O=(W,A,Z,M,Y)\\) For example, under identifiability assumptions the natural direct effect is equal to \\[\\begin{equation*} \\psi(\\P) = \\E[\\color{Goldenrod}{\\E\\{\\color{ForestGreen}{\\{E(Y \\mid A=1, M, W) - \\E(Y \\mid A=0, M, W)}\\mid A=0,W\\}}] \\end{equation*}\\] The notation \\(\\psi(\\P)\\) implies that the parameter is a function of \\(\\P\\) This means that we can compute it for any distribution \\(\\P\\) For example, if we know the true \\(\\P(W,A,M,Y)\\), we can comnpute the true value of the parameter by: Computing the conditional expectation \\(\\E(Y\\mid A=1,M=m,W=w)\\) for all values \\((m,w)\\) Computing the conditional expectation \\(\\E(Y\\mid A=0,M=m,W=w)\\) for all values \\((m,w)\\) Computing the probability \\(\\P(M=m\\mid A=0,W=w)\\) for all values \\((m,w)\\) Compute \\[\\begin{align*} \\color{Goldenrod}{\\E\\{}&amp;\\color{ForestGreen}{\\E(Y \\mid A=1, M, W) - \\E(Y \\mid A=0, M, W)}\\color{Goldenrod}{\\mid A=0,W\\}} =\\\\ &amp;\\color{Goldenrod}{\\sum_m\\color{ForestGreen}{\\{\\E(Y \\mid A=1, m, w) - \\E(Y \\mid A=0, m, w)\\}}\\P(M=m, A=0, W=w)} \\end{align*}\\] Computing the probability \\(\\P(W=w)\\) for all values \\(w\\) Computing the mean over all values \\(w\\) 5.1.2 Estimating identification formulas The above is how you would compute the true value if you know the true distribution \\(\\P\\) This is exactly what we did in our R examples before But we can use the same logic for estimation: Fit a regression to estimate, say \\(\\hat\\E(Y\\mid A=1,M=m,W=w)\\) Fit a regression to estimate, say \\(\\hat\\E(Y\\mid A=0,M=m,W=w)\\) Fit a regression to estimate, say \\(\\hat\\P(M=m\\mid A=0,W=w)\\) Estimate \\(\\P(W=w)\\) with the empirical distribution Evaluate \\[\\begin{equation*} \\psi(\\hat\\P) = \\hat\\E[\\hat\\E\\{\\hat\\E(Y \\mid A=1, M, W) - \\hat\\E(Y \\mid A=0, M, W)\\mid A=0,W\\}] \\end{equation*}\\] This is known as the g-computation estimator 5.1.3 How can g-estimation be implemented in practice? There are two possible ways to do g-computation estimation: Using parametric models for the above regressions Using flexible data-adaptive regression (aka machine learning) 5.1.4 Pros and cons of parametric models Pros: Easy to understand Ease of implementation (standard regression software) Can use the Delta method or the bootstrap for computation of standard errors Cons: Unless \\(W\\) and \\(M\\) contain very few categorical variables, it is very easy to misspecify the models This can introduce sizable bias in the estimators This modelling assumptions have become less necessary in the presence of data-adaptive regression tools (a.k.a machine learning) 5.1.5 An example of the bias of a g-computation estimator of the natural direct effect The following R chunk provides simulation code to exemplify the bias of a g-computation parametric estimator in a simple situation mean_y &lt;- function(m, a, w) abs(w) + a * m mean_m &lt;- function(a, w) plogis(w^2 - a) pscore &lt;- function(w) plogis(1 - abs(w)) This yields a true NDE value of 0.58048 w_big &lt;- runif(1e6, -1, 1) trueval &lt;- mean((mean_y(1, 1, w_big) - mean_y(1, 0, w_big)) * mean_m(0, w_big) + (mean_y(0, 1, w_big) - mean_y(0, 0, w_big)) * (1 - mean_m(0, w_big))) print(trueval) #&gt; [1] 0.58062 Let’s perform a simulation where we draw 1000 datasets from the above distribution, and compute a g-computation estimator based on gcomp &lt;- function(y, m, a, w) { lm_y &lt;- lm(y ~ m + a + w) pred_y1 &lt;- predict(lm_y, newdata = data.frame(a = 1, m = m, w = w)) pred_y0 &lt;- predict(lm_y, newdata = data.frame(a = 0, m = m, w = w)) pseudo &lt;- pred_y1 - pred_y0 lm_pseudo &lt;- lm(pseudo ~ a + w) pred_pseudo &lt;- predict(lm_pseudo, newdata = data.frame(a = 0, w = w)) estimate &lt;- mean(pred_pseudo) return(estimate) } estimate &lt;- lapply(seq_len(1000), function(iter) { n &lt;- 1000 w &lt;- runif(n, -1, 1) a &lt;- rbinom(n, 1, pscore(w)) m &lt;- rbinom(n, 1, mean_m(a, w)) y &lt;- rnorm(n, mean_y(m, a, w)) est &lt;- gcomp(y, m, a, w) return(est) }) estimate &lt;- do.call(c, estimate) hist(estimate) abline(v = trueval, col = &quot;red&quot;, lwd = 4) The bias also affects the confidence intervals: cis &lt;- cbind( estimate - qnorm(0.975) * sd(estimate), estimate + qnorm(0.975) * sd(estimate) ) ord &lt;- order(rowSums(cis)) lower &lt;- cis[ord, 1] upper &lt;- cis[ord, 2] curve(trueval + 0 * x, ylim = c(0, 1), xlim = c(0, 1001), lwd = 2, lty = 3, xaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;Confidence interval&quot;, cex.axis = 1.2, cex.lab = 1.2 ) for (i in 1:1000) { clr &lt;- rgb(0.5, 0, 0.75, 0.5) if (upper[i] &lt; trueval || lower[i] &gt; trueval) clr &lt;- rgb(1, 0, 0, 1) points(rep(i, 2), c(lower[i], upper[i]), type = &quot;l&quot;, lty = 1, col = clr) } text(450, 0.10, &quot;n=1000 repetitions = 1000 &quot;, cex = 1.2) text(450, 0.01, paste0( &quot;Coverage probability = &quot;, mean(lower &lt; trueval &amp; trueval &lt; upper), &quot;%&quot; ), cex = 1.2) 5.1.6 Pros and cons of g-computation with data-adaptive regression Pros: Easy to understand Alleviate model-misspecification bias Cons: Might be harder to implement depending on the regression procedures used No general approaches for computation of standard errors and confidence intervals For example, the bootstrap is not guaranteed to work, and it is known to fail in some cases 5.2 Semiparametric estimation Intuitively, it offers a way to use data-adaptive regression to avoid model misspecification bias, endow the estimators with additional robustness (e.g., double robustness), while allowing the computation of correct standard errors and confidence intervals This can be achieved by adding a bias correction factor the the g-computation as follows: \\[\\begin{equation*} \\psi(\\hat \\P) + \\frac{1}{n}\\sum_{i=1}^n D(O_i) \\end{equation*}\\] for some function \\(D(O_i)\\) of the data The function \\(D(O)\\) is called the efficient influence function (EIF) The EIF must be found on a case-by-case basis for each parameter \\(\\psi(\\P)\\) For example, for estimating the standardized mean \\(\\psi(\\P)=\\E[\\E(Y\\mid A=1, W)]\\), we have \\[\\begin{equation*} D(O) = \\frac{A}{\\hat \\P(A=1\\mid W)}[Y - \\hat\\E(Y\\mid A=1, W)] + \\hat\\E(Y\\mid A=1, W) - \\psi(\\hat\\P) \\end{equation*}\\] The EIF is found by using a distributional analogue of a Taylor expansion In this workshop we will omit the specific form of \\(D(O)\\) for some of the parameters that we use But the estimators we discuss and implement in the R packages will be based on these EIFs And the specific form of the EIF may be found in papers in the references Note: the bias correction above may have an additional problem of returning parameter estimates outside of natural bounds. E.g., probabilities greater than one. A solution to this (not discussed in this workshop but implemented in some of the R packages) is targeted minimum loss based estimation. "],["using-the-eif-to-construct-an-estimator-the-case-of-the-natural-direct-effect.html", "Chapter 6 Using the EIF to construct an estimator: the case of the natural direct effect 6.1 Natural direct effect", " Chapter 6 Using the EIF to construct an estimator: the case of the natural direct effect 6.1 Natural direct effect Recall: FIGURE 2.1: Directed acyclic graph under no intermediate confounders of the mediator-outcome relation affected by treatment Assuming a binary \\(A\\), we define the natural direct effect as: \\[NDE = E(Y_{1,M_{0}} - Y_{0,M_{0}}),\\] and the natural indirect effect as: \\[NIE = E(Y_{1,M_{1}} - Y_{1,M_{0}}).\\] The observed data is \\(O=(W, A, M, Y)\\) This SCM is represented in the above DAG and the following causal models: \\[\\begin{align*} W &amp; = f_W(U_W)\\\\ A &amp; = f_A(W, U_A)\\\\ M &amp; = f_M(W, A, U_M)\\\\ Y &amp; = f_Y(W, A, M, U_Y), \\end{align*}\\] where \\((U_W, U_A,U_M, U_Y)\\) are exogenous random errors. We assume - \\(A\\) is a single binary randomized treatment (and thus \\(A = f_A(U_A)\\)) - \\(M\\) is a single binary mediator - There are no restrictions on the distribution of \\(W\\) or \\(Y\\) Recall that we need to assume the following to identify the above caual effects from our observed data: \\(A \\indep Y_{a,m} \\mid W\\) \\(M \\indep Y_{a,m} \\mid W, A\\) \\(A \\indep M_a \\mid W\\) \\(M_0 \\indep Y_{1,m} \\mid W\\) and positivity assumptions Then, the NDE is identified as \\[\\begin{equation*} \\psi(\\P) = \\E[\\E\\{\\E(Y \\mid A=1, M, W) - \\E(Y \\mid A=0, M, W)\\mid A=0,W\\}] \\end{equation*}\\] 6.1.1 The efficient influence function for the NDE For illustration, we will first present how to construct an estimator of the NDE that uses the EIF ``by hand’’ For other parameters, we will teach you how to use our packages medoutcon and medshift First, we need to introduce some notation to describe the EIF for the NDE Let \\(Q(M, W)\\) denote \\(\\E(Y\\mid A=1, M, W) - \\E(Y\\mid A=0, M, W)\\) We can now introduce the EIF: \\[\\begin{align*} D(O) &amp;= \\color{ForestGreen}{\\bigg\\{ \\frac{I(A=1)}{\\P(A=1\\mid W)}\\frac{\\P(M\\mid A=0,W)}{\\P(M\\mid A=1,W)} - \\frac{I(A=0)}{\\P(A=0\\mid W)}\\bigg\\}} \\times \\color{Goldenrod}{[Y-\\E(Y\\mid A,M,W)]} \\\\ &amp;+ \\color{ForestGreen}{\\frac{I(A=0)}{\\P(A=0\\mid W)}}\\color{Goldenrod}{\\big\\{Q(M,W) - \\E[Q(M,W) | W,A=0] \\big\\}}\\\\ &amp;+ \\color{Goldenrod}{\\E[Q(M,W) | W,A=0] - \\psi(\\P)} \\end{align*}\\] Estimating \\(\\P(M\\mid A, W)\\) is a really hard problem when \\(M\\) is high-dimensional. But, since we have the ratio of these conditional densitities, we can reparamterize using Bayes rule to get something that is easier to compute: \\[\\begin{equation*} \\frac{\\P(M\\mid A=0,W)}{\\P(M\\mid A=1,W)} = \\frac{\\P(A = 0 \\mid M, W) \\P(A=1 \\mid W)}{\\P(A = 1 \\mid M, W)\\P(A=0 \\mid W)}. \\end{equation*}\\] Thus we can change the expression of the EIF a bit as follows. First, some more notation that will be useful later: Let \\(g(a\\mid w)\\) denote \\(\\P(A=a\\mid W=w)\\) Let \\(e(a\\mid m, w)\\) denote \\(\\P(A=a\\mid M=m, W=w)\\) Let \\(b(a, m, w)\\) denote \\(\\E(Y\\mid A=a, M=m, W=w)\\) The EIF is \\[\\begin{align*} D(O) &amp;= \\color{ForestGreen}{\\bigg\\{ \\frac{I(A=1)}{g(0\\mid W)}\\frac{e(0\\mid M,W)}{e(1\\mid M,W)} - \\frac{I(A=0)}{g(0\\mid W)}\\bigg\\}} \\times \\color{Goldenrod}{[Y-b(A,M,W)]} \\\\ &amp;+ \\color{ForestGreen}{\\frac{I(A=0)}{g(0\\mid W)}}\\color{Goldenrod}{\\big\\{Q(M,W) - \\E[Q(M,W) | W,A=0] \\big\\}}\\\\ &amp;+ \\color{Goldenrod}{\\E[Q(M,W) | W,A=0] - \\psi(\\P)} \\end{align*}\\] 6.1.2 How to compute the one-step estimator (akin to Augmented IPW) First we will generate some data: mean_y &lt;- function(m, a, w) abs(w) + a * m mean_m &lt;- function(a, w)plogis(w^2 - a) pscore &lt;- function(w) plogis(1 - abs(w)) w_big &lt;- runif(1e6, -1, 1) trueval &lt;- mean((mean_y(1, 1, w_big) - mean_y(1, 0, w_big)) * mean_m(0, w_big) + (mean_y(0, 1, w_big) - mean_y(0, 0, w_big)) * (1 - mean_m(0, w_big))) n &lt;- 1000 w &lt;- runif(n, -1, 1) a &lt;- rbinom(n, 1, pscore(w)) m &lt;- rbinom(n, 1, mean_m(a, w)) y &lt;- rnorm(n, mean_y(m, a, w)) Recall that the one-step estimator is defined as the bias-corrected g-computation estimator: \\[\\begin{equation*} \\psi(\\hat \\P) + \\frac{1}{n}\\sum_{i=1}^n D(O;\\hat \\P_i) \\end{equation*}\\] Can be computed in the following steps: Fit models for \\(g(a\\mid w)\\), \\(e(a\\mid m, w)\\), and \\(b(a, m, w)\\) In this example we will use Generalized Additive Models for tractability In applied settings we recommend using an ensemble of data-adaptive regression algorithms, such as the Super Learner (van der Laan, Polley, and Hubbard 2007) library(mgcv) ## fit model for E(Y | A, W) b_fit &lt;- gam(y ~ m:a + s(w, by = a)) ## fit model for P(A = 1 | M, W) e_fit &lt;- gam(a ~ m + w + s(w, by = m), family = binomial) ## fit model for P(A = 1 | W) g_fit &lt;- gam(a ~ w, family = binomial) Compute predictions \\(g(1\\mid w)\\), \\(g(0\\mid w)\\), \\(e(1\\mid m, w)\\), \\(e(0\\mid m, w)\\),\\(b(1, m, w)\\), \\(b(0, m, w)\\), and \\(b(a, m, w)\\) ## Compute P(A = 1 | W) g1_pred &lt;- predict(g_fit, type = &#39;response&#39;) ## Compute P(A = 0 | W) g0_pred &lt;- 1 - g1_pred ## Compute P(A = 1 | M, W) e1_pred &lt;- predict(e_fit, type = &#39;response&#39;) ## Compute P(A = 0 | M, W) e0_pred &lt;- 1 - e1_pred ## Compute E(Y | A = 1, M, W) b1_pred &lt;- predict(b_fit, newdata = data.frame(a = 1, m, w)) ## Compute E(Y | A = 0, M, W) b0_pred &lt;- predict(b_fit, newdata = data.frame(a = 0, m, w)) ## Compute E(Y | A, M, W) b_pred &lt;- predict(b_fit) Compute \\(Q(M, W)\\), fit a model for \\(\\E[Q(M,W) | W,A]\\), and predict at \\(A=0\\) ## Compute Q(M, W) pseudo &lt;- b1_pred - b0_pred ## Fit model for E[Q(M, W) | A, W] q_fit &lt;- gam(pseudo ~ a + w + s(w, by = a)) ## Compute E[Q(M, W) | A = 0, W] q_pred &lt;- predict(q_fit, newdata = data.frame(a = 0, w = w)) Estimate the weights \\[\\begin{equation*} \\bigg\\{ \\frac{I(A=1)}{g(0\\mid W)}\\frac{e(0\\mid M,W)}{e(1\\mid M,W)} - \\frac{I(A=0)}{g(0\\mid W)}\\bigg\\} \\end{equation*}\\] using the above predictions: ip_weights &lt;- a / g0_pred * e0_pred / e1_pred - (1 - a) / g0_pred Compute the uncentered EIF: eif &lt;- ip_weights * (y - b_pred) + (1 - a) / g0_pred * (pseudo - q_pred) + q_pred The one step estimator is the mean of the uncentered EIF ## One-step estimator mean(eif) #&gt; [1] 0.55085 6.1.3 Performance of the one-step estimator in a small simulation study First, we create a wrapper around the estimator one_step &lt;- function(y, m, a, w) { b_fit &lt;- gam(y ~ m:a + s(w, by = a)) e_fit &lt;- gam(a ~ m + w + s(w, by = m), family = binomial) g_fit &lt;- gam(a ~ w, family = binomial) g1_pred &lt;- predict(g_fit, type = &#39;response&#39;) g0_pred &lt;- 1 - g1_pred e1_pred &lt;- predict(e_fit, type = &#39;response&#39;) e0_pred &lt;- 1 - e1_pred b1_pred &lt;- predict(b_fit, newdata = data.frame(a = 1, m, w), type = &#39;response&#39;) b0_pred &lt;- predict(b_fit, newdata = data.frame(a = 0, m, w), type = &#39;response&#39;) b_pred &lt;- predict(b_fit, type = &#39;response&#39;) pseudo &lt;- b1_pred - b0_pred q_fit &lt;- gam(pseudo ~ a + w + s(w, by = a)) q_pred &lt;- predict(q_fit, newdata = data.frame(a = 0, w = w)) ip_weights &lt;- a / g0_pred * e0_pred / e1_pred - (1 - a) / g0_pred eif &lt;- ip_weights * (y - b_pred) + (1 - a) / g0_pred * (pseudo - q_pred) + q_pred return(mean(eif)) } Let us first examine the bias The true value is: w_big &lt;- runif(1e6, -1, 1) trueval &lt;- mean((mean_y(1, 1, w_big) - mean_y(1, 0, w_big)) * mean_m(0, w_big) + (mean_y(0, 1, w_big) - mean_y(0, 0, w_big)) * (1 - mean_m(0, w_big))) print(trueval) #&gt; [1] 0.58061 Bias simulation estimate &lt;- lapply(seq_len(1000), function(iter) { n &lt;- 1000 w &lt;- runif(n, -1, 1) a &lt;- rbinom(n, 1, pscore(w)) m &lt;- rbinom(n, 1, mean_m(a, w)) y &lt;- rnorm(n, mean_y(m, a, w)) estimate &lt;- one_step(y, m, a, w) return(estimate) }) estimate &lt;- do.call(c, estimate) hist(estimate) abline(v = trueval, col = &quot;red&quot;, lwd = 4) And now the confidence intervals: cis &lt;- cbind( estimate - qnorm(0.975) * sd(estimate), estimate + qnorm(0.975) * sd(estimate) ) ord &lt;- order(rowSums(cis)) lower &lt;- cis[ord, 1] upper &lt;- cis[ord, 2] curve(trueval + 0 * x, ylim = c(0, 1), xlim = c(0, 1001), lwd = 2, lty = 3, xaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;Confidence interval&quot;, cex.axis = 1.2, cex.lab = 1.2 ) for (i in 1:1000) { clr &lt;- rgb(0.5, 0, 0.75, 0.5) if (upper[i] &lt; trueval || lower[i] &gt; trueval) clr &lt;- rgb(1, 0, 0, 1) points(rep(i, 2), c(lower[i], upper[i]), type = &quot;l&quot;, lty = 1, col = clr) } text(450, 0.10, &quot;n=1000 repetitions = 1000 &quot;, cex = 1.2) text(450, 0.01, paste0( &quot;Coverage probability = &quot;, mean(lower &lt; trueval &amp; trueval &lt; upper), &quot;%&quot; ), cex = 1.2) 6.1.4 A note about targeted minimum loss-based estimation (TMLE) The above estimator is great because it allows us to use data-adaptive regression to avoid bias, while allowing the computation of correct standard errors This estimator has one problem: It can yield answers outside of the bounds of the parameter space E.g., if \\(Y\\) is binary, it could yield direct and indirect effects outside of \\([-1,1]\\) To solve this, you can compute a TMLE instead (implemented in the R packages, coming up) 6.1.5 A note about cross-fitting When using data-adaptive regression estimators, it is recommended to use cross-fitted estimators Cross-fitting is similar to cross-validation: Randomly split the sample into K (e.g., K=10) subsets of equal size For each of the 9/10ths of the sample, fit the regression models Use the out-of-sample fit to predict in the remaining 1/10th of the sample Cross-fitting further reduces the bias of the estimators Cross-fitting aids in guaranteeing the correctness of the standard errors and confidence intervals Cross-fitting is implemented by default in the R packages that you will see next "],["r-packages-for-estimation-of-the-causal-indirect-effects.html", "Chapter 7 R packages for estimation of the causal (in)direct effects 7.1 medoutcon: Natural and interventional (in)direct effects 7.2 medshift: Stochastic (in)direct effects", " Chapter 7 R packages for estimation of the causal (in)direct effects We’ll now turn to working through a few examples of estimating the natural, interventional, and stochastic direct and indirect effects. As our running example, we’ll a simple data set from an observational study of the relationship between BMI and kids’ behavior, freely distributed with the mma R package on CRAN. First, let’s load the packages we’ll be using and set a seed; then, load this data set and take a quick look library(tidyverse) library(sl3) library(medoutcon) library(medshift) library(mma) set.seed(429153) # load and examine data data(weight_behavior) dim(weight_behavior) #&gt; [1] 691 15 # drop missing values weight_behavior &lt;- weight_behavior %&gt;% drop_na() %&gt;% as_tibble() weight_behavior #&gt; # A tibble: 567 x 15 #&gt; bmi age sex race numpeople car gotosch snack tvhours cmpthours #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 18.2 12.2 F OTHER 5 3 2 1 4 0 #&gt; 2 22.8 12.8 M OTHER 4 3 2 1 4 2 #&gt; 3 25.6 12.1 M OTHER 2 3 2 1 0 2 #&gt; 4 15.1 12.3 M OTHER 4 1 2 1 2 1 #&gt; 5 23.0 11.8 M OTHER 4 1 1 1 4 3 #&gt; # … with 562 more rows, and 5 more variables: cellhours &lt;dbl&gt;, sports &lt;fct&gt;, #&gt; # exercises &lt;int&gt;, sweat &lt;int&gt;, overweigh &lt;dbl&gt; The documentation for the data set describes it as a “database obtained from the Louisiana State University Health Sciences Center, New Orleans, by Dr. Richard Scribner. He explored the relationship between BMI and kids’ behavior through a survey at children, teachers and parents in Grenada in 2014. This data set includes 691 observations and 15 variables.” Note that the data set contained several observations with missing values, which we removed above to simplify the demonstration of our analytic methods. In practice, we recommend instead using appropriate corrections (e.g., imputation, inverse weighting) to fully take advantage of the observed data. Following the motivation of the original study, we focus on the causal effects of participating in a sports team (sports) on the BMI of children (bmi), taking into consideration several mediators (snack, exercises, overweigh); all other measured covariates are taken to be potential baseline confounders. 7.1 medoutcon: Natural and interventional (in)direct effects The data on a single observational unit can be represented \\(O = (W, A, M, Y)\\), with the data pooled across all participants denoted \\(O_1, \\ldots, O_n\\), for a of \\(n\\) i.i.d. observations of \\(O\\). Recall the DAG from an earlier chapter, which represents the data-generating process: FIGURE 2.1: Directed acyclic graph under no intermediate confounders of the mediator-outcome relation affected by treatment 7.1.1 Natural (in)direct effects To start, we will consider estimation of the natural direct and indirect effects, which, we recall, are defined as follows \\[\\begin{equation*} \\E[Y_{1,M_1} - Y_{0,M_0}] = \\underbrace{\\E[Y_{\\color{red}{1},\\color{blue}{M_1}} - Y_{\\color{red}{1},\\color{blue}{M_0}}]}_{\\text{natural indirect effect}} + \\underbrace{\\E[Y_{\\color{blue}{1},\\color{red}{M_0}} - Y_{\\color{blue}{0},\\color{red}{M_0}}]}_{\\text{natural direct effect}}. \\end{equation*}\\] Our medoutcon R package (Hejazi, Dı́az, and Rudolph 2021), which accompanies Dı́az et al. (2020), implements one-step and TML estimators of both the natural and interventional (in)direct effects. Both types of estimators are capable of accommodating flexible modeling strategies (e.g., ensemble machine learning) for the initial estimation of nuisance parameters. The medoutcon R package uses cross-validation in initial estimation: this results in cross-validated (or “cross-fitted”) one-step and TML estimators (Klaassen 1987; Zheng and van der Laan 2011; Chernozhukov et al. 2018), which exhibit greater robustness than their non-sample-splitting analogs. To this end, medoutcon integrates with the sl3 R package, which is extensively documented in this book chapter (van der Laan et al. 2022). 7.1.2 Interlude: sl3 for nuisance parameter estimation To fully take advantage of the one-step and TML estimators, we’d like to rely on flexible, data adaptive strategies for nuisance parameter estimation. Doing so minimizes opportunities for model misspecification to compromise our analytic conclusions. Choosing among the diversity of available machine learning algorithms can be challenging, so we recommend using the Super Learner algorithm for ensemble machine learning (van der Laan, Polley, and Hubbard 2007), which is implemented in the sl3 R package (Coyle et al. 2021). Below, we demonstrate the construction of an ensemble learner based on a limited library of algorithms, including n intercept model, a main terms GLM, Lasso (\\(\\ell_1\\)-penalized) regression, and random forest (ranger). # instantiate learners mean_lrnr &lt;- Lrnr_mean$new() fglm_lrnr &lt;- Lrnr_glm_fast$new() lasso_lrnr &lt;- Lrnr_glmnet$new(alpha = 1, nfolds = 3) rf_lrnr &lt;- Lrnr_ranger$new(num.trees = 200) # create learner library and instantiate super learner ensemble lrnr_lib &lt;- Stack$new(mean_lrnr, fglm_lrnr, lasso_lrnr, rf_lrnr) sl_lrnr &lt;- Lrnr_sl$new(learners = lrnr_lib, metalearner = Lrnr_nnls$new()) Of course, there are many alternatives for learning algorithms to be included in such a modeling library. Feel free to explore! 7.1.3 Efficient estimation of the natural (in)direct effects Estimation of the natural direct and indirect effects requires estimation of a few nuisance parameters. Recall that these are \\(g(a\\mid w)\\), which denotes \\(\\P(A=a \\mid W=w)\\) \\(h(a\\mid m, w)\\), which denotes \\(\\P(A=a \\mid M=m, W=w)\\) \\(b(a, m, w)\\), which denotes \\(\\E(Y \\mid A=a, M=m, W=w)\\) While we recommend the use of Super Learning, we opt to instead estimate all nuisance parameters with Lasso regression below (to save computational time). Now, let’s use the medoutcon() function to estimate the natural direct effect: # compute one-step estimate of the natural direct effect nde_onestep &lt;- medoutcon( W = weight_behavior[, c(&quot;age&quot;, &quot;sex&quot;, &quot;race&quot;, &quot;tvhours&quot;)], A = (as.numeric(weight_behavior$sports) - 1), Z = NULL, M = weight_behavior[, c(&quot;snack&quot;, &quot;exercises&quot;, &quot;overweigh&quot;)], Y = weight_behavior$bmi, g_learners = lasso_lrnr, h_learners = lasso_lrnr, b_learners = lasso_lrnr, effect = &quot;direct&quot;, estimator = &quot;onestep&quot;, estimator_args = list(cv_folds = 5) ) summary(nde_onestep) #&gt; # A tibble: 1 x 7 #&gt; lwr_ci param_est upr_ci var_est eif_mean estimator param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 -0.490 -0.0280 0.434 0.0555 2.84e-15 onestep direct_natural We can similarly call medoutcon() to estimate the natural indirect effect: # compute one-step estimate of the natural indirect effect nie_onestep &lt;- medoutcon( W = weight_behavior[, c(&quot;age&quot;, &quot;sex&quot;, &quot;race&quot;, &quot;tvhours&quot;)], A = (as.numeric(weight_behavior$sports) - 1), Z = NULL, M = weight_behavior[, c(&quot;snack&quot;, &quot;exercises&quot;, &quot;overweigh&quot;)], Y = weight_behavior$bmi, g_learners = lasso_lrnr, h_learners = lasso_lrnr, b_learners = lasso_lrnr, effect = &quot;indirect&quot;, estimator = &quot;onestep&quot;, estimator_args = list(cv_folds = 5) ) summary(nie_onestep) #&gt; # A tibble: 1 x 7 #&gt; lwr_ci param_est upr_ci var_est eif_mean estimator param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.466 1.09 1.72 0.102 9.02e-16 onestep indirect_natural From the above, we can conclude that the effect of participation on a sports team on BMI is primarily mediated by the variables snack, exercises, and overweigh, as the natural indirect effect is several times larger than the natural direct effect. Note that we could have instead used the TML estimators, which have improved finite-sample performance, instead of the one-step estimators. Doing this is as simple as setting the estimator = \"tmle\" in the relevant argument. 7.1.4 Interventional (in)direct effects Since our knowledge of the system under study is incomplete, we might worry that one (or more) of the measured variables are not mediators, but, in fact, intermediate confounders affected by treatment. While the natural (in)direct effects are not identified in this setting, their interventional (in)direct counterparts are, as we saw in an earlier section. Recall that both types of effects are defined by static interventions on the treatment. The interventional effects are distinguished by their use of a stochastic intervention on the mediator to aid in their identification. FIGURE 7.1: Directed acyclic graph under intermediate confounders of the mediator-outcome relation affected by treatment Recall that the interventional (in)direct effects are defined via the decomposition: \\[\\begin{equation*} \\E[Y_{1,G_1} - Y_{0,G_0}] = \\underbrace{\\E[Y_{\\color{red}{1},\\color{blue}{G_1}} - Y_{\\color{red}{1},\\color{blue}{G_0}}]}_{\\text{interventional indirect effect}} + \\underbrace{\\E[Y_{\\color{blue}{1},\\color{red}{G_0}} - Y_{\\color{blue}{0},\\color{red}{G_0}}]}_{\\text{interventional direct effect}} \\end{equation*}\\] In our data example, we’ll consider the eating of snacks as a potential intermediate confounder, since one might reasonably hypothesize that participation on a sports team might subsequently affect snacking, which then could affect mediators like the amount of exercises and overweight status. The interventional direct and indirect effects may also be easily estimated with the medoutcon R package (Hejazi, Dı́az, and Rudolph 2021). Just as for the natural (in)direct effects, medoutcon implements cross-validated one-step and TML estimators of the interventional effects. 7.1.5 Efficient estimation of the interventional (in)direct effects Estimation of these effects is more complex, so a few additional nuisance parameters arise when expressing the (more general) EIF for these effects: \\(q(z \\mid a, w)\\), the conditional density of the intermediate confounders, conditional only on treatment and baseline covariates; \\(r(z \\mid a, m, w)\\), the conditional density of the intermediate confounders, conditional on mediators, treatment, and baseline covariates. To estimate the interventional effects, we only need to set the argument Z of medoutcon to a value other than NULL. Note that the implementation in medoutcon is currently limited to settings with only binary intermediate confounders, i.e., \\(Z \\in \\{0, 1\\}\\). Let’s use medoutcon() to estimate the interventional direct effect: # compute one-step estimate of the interventional direct effect interv_de_onestep &lt;- medoutcon( W = weight_behavior[, c(&quot;age&quot;, &quot;sex&quot;, &quot;race&quot;, &quot;tvhours&quot;)], A = (as.numeric(weight_behavior$sports) - 1), Z = (as.numeric(weight_behavior$snack) - 1), M = weight_behavior[, c(&quot;exercises&quot;, &quot;overweigh&quot;)], Y = weight_behavior$bmi, g_learners = lasso_lrnr, h_learners = lasso_lrnr, b_learners = lasso_lrnr, effect = &quot;direct&quot;, estimator = &quot;onestep&quot;, estimator_args = list(cv_folds = 5) ) summary(interv_de_onestep) #&gt; # A tibble: 1 x 7 #&gt; lwr_ci param_est upr_ci var_est eif_mean estimator param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 -0.476 -0.0107 0.454 0.0562 -9.93e-16 onestep direct_interventional We can similarly estimate the interventional indirect effect: # compute one-step estimate of the interventional indirect effect interv_ie_onestep &lt;- medoutcon( W = weight_behavior[, c(&quot;age&quot;, &quot;sex&quot;, &quot;race&quot;, &quot;tvhours&quot;)], A = (as.numeric(weight_behavior$sports) - 1), Z = (as.numeric(weight_behavior$snack) - 1), M = weight_behavior[, c(&quot;exercises&quot;, &quot;overweigh&quot;)], Y = weight_behavior$bmi, g_learners = lasso_lrnr, h_learners = lasso_lrnr, b_learners = lasso_lrnr, effect = &quot;indirect&quot;, estimator = &quot;onestep&quot;, estimator_args = list(cv_folds = 5) ) summary(interv_ie_onestep) #&gt; # A tibble: 1 x 7 #&gt; lwr_ci param_est upr_ci var_est eif_mean estimator param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.348 0.952 1.56 0.0950 3.15e-15 onestep indirect_interventional From the above, we can conclude that the effect of participation on a sports team on BMI is largely through the interventional indirect effect (i.e., through the pathways involving the mediating variables) rather than via its direct effect. Just as before, we could have instead used the TML estimators, instead of the one-step estimators. Doing this is as simple as setting the estimator = \"tmle\" in the relevant argument. 7.2 medshift: Stochastic (in)direct effects While the analyses using the natural and interventional effects have been illuminating, we may also go beyond the restrictive static interventions required to define these (in)direct effects. In fact, it may be more realistic to consider interventions that do not directly force children to join athletic teams, but instead motivate them to make their participation on such teams more likely. Importantly, such interventions are often far more realistic and actionable in real-world studies. 7.2.1 Formulating the stochastic (in)direct effects These more flexible intervention regimes are incompatible with (in)direct effect definitions based on decomposing the average treatment effect. Instead, consider the decomposition of the population intervention effect (PIE) of a stochastic intervention into direct and indirect effects (Dı́az and Hejazi 2020): \\[\\begin{align*} \\E[Y&amp;_{A_\\delta,M_{A_\\delta}} - Y_{A,M_A}] = \\\\ &amp;\\underbrace{\\E[Y_{\\color{red}{A_\\delta},\\color{blue}{M_{A_\\delta}}} - Y_{\\color{red}{A_\\delta},\\color{blue}{M}}]}_{\\text{stochastic natural indirect effect}} + \\underbrace{\\E[Y_{\\color{blue}{A_\\delta},\\color{red}{M}} - Y_{\\color{blue}{A},\\color{red}{M}}]}_{\\text{stochastic natural direct effect}} \\end{align*}\\] Recall from our discussion of the incremental propensity score interventions (Kennedy 2018) that such stochastic interventions can compare the pre- and post-intervention odds of exposure: \\[\\begin{equation*} \\delta = \\frac{\\text{odds}(A_\\delta = 1\\mid W=w)} {\\text{odds}(A = 1\\mid W=w)}. \\end{equation*}\\] In our analysis, we will modulate the odds of participating in a sports team by a fixed amount for each individual, setting, for example, \\(\\delta = 2\\): delta_shift_ipsi &lt;- 2 Such an intervention may be interpreted as the effect of a school program that motivates children to participate in sports teams. 7.2.2 Efficient estimation of the stochastic (in)direct effects The decomposition of the PIE into the direct and indirect effects leads to a common term \\(\\E[Y_{\\color{red}{A_\\delta},\\color{blue}{M}}]\\) involved in both the direct and indirect effect definitions. This term may be estimated via the medshift R package (Hejazi and Dı́az 2020). For the direct effect, the remaining term is the \\(\\E[Y_{\\color{blue}{A},\\color{red}{M}}]\\), which may be estimated by a simple mean in the observed data (i.e., no intervention). For the indirect effect, the remaining term is the joint effect of stochastic interventions on both \\(A\\) and \\(M\\): \\(\\E[Y_{\\color{red}{A_\\delta},\\color{blue}{M_{A_\\delta}}}\\). For the case of an IPSI on binary \\(A\\), this may be estimated by the tools in the npcausal R package. For the case of an MTP on continuous \\(A\\), this may be estimated by the tools in the txshift R package (Hejazi and Benkeser 2020a, 2020b). Like the implementation in medoutcon, the medshift package makes use of cross-validation in constructing initial estimates of nuisance parameters, resulting in more robust, cross-validated efficient estimators (Klaassen 1987; Zheng and van der Laan 2011; Chernozhukov et al. 2018). Now, we’re ready to use the medshift function to estimate the decomposition term common to both the stochastic direct and indirect effects: # compute one-step estimate of the decomposition term of the (in)direct effects stoch_decomp_onestep &lt;- medshift( W = weight_behavior[, c(&quot;age&quot;, &quot;sex&quot;, &quot;race&quot;, &quot;tvhours&quot;)], A = (as.numeric(weight_behavior$sports) - 1), Z = weight_behavior[, c(&quot;snack&quot;, &quot;exercises&quot;, &quot;overweigh&quot;)], Y = weight_behavior$bmi, delta = delta_shift_ipsi, g_learners = lasso_lrnr, e_learners = lasso_lrnr, m_learners = lasso_lrnr, estimator = &quot;onestep&quot;, estimator_args = list(cv_folds = 5) ) summary(stoch_decomp_onestep) #&gt; lwr_ci param_est upr_ci param_var eif_mean estimator #&gt; 18.770026 19.103221 19.436415 0.0289 -1.7263e-15 onestep To estimate the stochastic direct effect, an extra step is necessary – we must apply the delta method: # convenience function to compute inference via delta method: EY1 - EY0 linear_contrast &lt;- function(params, eifs, ci_level = 0.95) { # bounds for confidence interval ci_norm_bounds &lt;- c(-1, 1) * abs(stats::qnorm(p = (1 - ci_level) / 2)) param_est &lt;- params[[1]] - params[[2]] eif &lt;- eifs[[1]] - eifs[[2]] se_eif &lt;- sqrt(var(eif) / length(eif)) param_ci &lt;- param_est + ci_norm_bounds * se_eif # parameter and inference out &lt;- c(param_ci[1], param_est, param_ci[2]) names(out) &lt;- c(&quot;lwr_ci&quot;, &quot;param_est&quot;, &quot;upr_ci&quot;) return(out) } Straightforward application of this procedure yields, # parameter estimates and EIFs for components of direct effect EY &lt;- mean(weight_behavior$bmi) eif_EY &lt;- weight_behavior$bmi - EY params_de &lt;- list(stoch_decomp_onestep$theta, EY) eifs_de &lt;- list(stoch_decomp_onestep$theta, eif_EY) # direct effect = EY - estimated quantity de_est &lt;- linear_contrast(params_de, eifs_de) de_est #&gt; lwr_ci param_est upr_ci #&gt; -0.347205 -0.023887 0.299430 From the above, we can conclude that the effect of increasing the odds of participation on a sports team on BMI leads only to a relatively small direct effect. "],["references.html", "References", " References "]]
