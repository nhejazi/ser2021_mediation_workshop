[{"path":"index.html","id":"welcome-to-ser","chapter":"Welcome to SER!","heading":"Welcome to SER!","text":"open source, reproducible vignette half-day workshop modern\nmethods causal mediation analysis, given SER 2021 Meeting \nMonday, 24 May 2021.","code":""},{"path":"about-this-workshop.html","id":"about-this-workshop","chapter":"About this workshop","heading":"About this workshop","text":"Causal mediation analysis can provide mechanistic understanding \nexposure impacts outcome, central goal epidemiology health sciences.\nHowever, rapid methodologic developments coupled formal courses\npresents challenges implementation. Beginning overview classical\ndirect indirect effects, workshop present recent advances \novercome limitations previous methods, allowing : () continuous\nexposures, (ii) multiple, non-independent mediators, (iii) effects\nidentifiable presence intermediate confounders affected exposure.\nEmphasis placed flexible, stochastic interventional direct \nindirect effects, highlighting may applied answer substantive\nepidemiological questions real-world studies. Multiply robust,\nnonparametric estimators causal effects, free open source R\npackages (medshift \nmedoutcon) application, \nintroduced.ensure translation real-world data analysis, workshop \nincorporate hands-R programming exercises allow participants practice \nimplementing statistical tools presented. recommended \nparticipants working knowledge basic notions causal inference,\nincluding counterfactuals identification (linking causal effect \nparameter estimable observed data distribution). Familiarity \nR programming language also recommended.","code":""},{"path":"about-this-workshop.html","id":"schedule","chapter":"About this workshop","heading":"0.1 Workshop schedule","text":"10:00A-10:30A: introductions/mediation set up10:30A-11:00A: estimands choose11:00A-11:30A: discussion: choose real-world examples11:30A-12:00P: shift parameter introduction application lecture part12:00P-12:15P break/discussion12:15P-12:45P estimation natural direct indirect effects,\ninterventional direct indirect effects12:45P-01:15P: practice R code estimation01:15P-01:30P: estimation stochastic interventional direct indirect\neffects01:30P-01:50P: practice: code estimation01:50P-02:00P wrap upNOTE: times listed Pacific Time.","code":""},{"path":"about-this-workshop.html","id":"about-the-instructors","chapter":"About this workshop","heading":"About the instructors","text":"","code":""},{"path":"about-this-workshop.html","id":"iván-díaz","chapter":"About this workshop","heading":"Iván Díaz","text":"research focuses development non-parametric statistical methods \ncausal inference observational randomized studies complex\ndatasets, using machine learning. includes limited mediation\nanalysis, methods continuous exposures, longitudinal data including survival\nanalysis, efficiency guarantees covariate adjustment randomized\ntrials. also interested general semi-parametric theory, machine\nlearning, high-dimensional data.","code":""},{"path":"about-this-workshop.html","id":"nima-hejazi","chapter":"About this workshop","heading":"Nima Hejazi","text":"PhD candidate biostatistics UC Berkeley, working joint\ndirection Mark van der Laan Alan Hubbard. research interests fall \nintersection causal inference machine learning, drawing ideas \nnon/semi-parametric estimation large, flexible statistical models develop\nefficient robust statistical procedures evaluating complex target\nestimands observational randomized studies. Particular areas current\nemphasis include causal mediation/path analysis, outcome-dependent sampling\ndesigns, targeted loss-based estimation, applications vaccine efficacy\ntrials. also passionate statistical computing open source\nsoftware development applied statistics.","code":""},{"path":"about-this-workshop.html","id":"kara-rudolph","chapter":"About this workshop","heading":"Kara Rudolph","text":"Assistant Professor Epidemiology Columbia University. research\ninterests developing applying causal inference methods understand\nsocial contextual influences mental health, substance use, violence\ndisadvantaged, urban areas United States. current work focuses \ndeveloping methods transportability mediation, subsequently applying\nmethods understand aspects school peer environments\nmediate relationships neighborhood factors adolescent drug use\nacross populations. generally, work generalizing/ transporting\nfindings study samples target populations identifying subpopulations\nlikely benefit interventions contributes efforts optimally\ntarget available policy program resources.","code":""},{"path":"about-this-workshop.html","id":"repro","chapter":"About this workshop","heading":"0.2 Reproduciblity","text":"workshop materials written using bookdown,\ncomplete source available \nGitHub. version book\nbuilt R version 4.0.4 (2021-02-15), pandoc version r rmarkdown::pandoc_version(), following packages:","code":""},{"path":"about-this-workshop.html","id":"setup","chapter":"About this workshop","heading":"0.3 Setup instructions","text":"","code":""},{"path":"about-this-workshop.html","id":"r-and-rstudio","chapter":"About this workshop","heading":"0.3.1 R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"about-this-workshop.html","id":"windows","chapter":"About this workshop","heading":"0.3.1.1 Windows","text":"","code":""},{"path":"about-this-workshop.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"About this workshop","heading":"0.3.1.1.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"about-this-workshop.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"About this workshop","heading":"0.3.1.1.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download\npageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"about-this-workshop.html","id":"macos-mac-os-x","chapter":"About this workshop","heading":"0.3.1.2 macOS / Mac OS X","text":"","code":""},{"path":"about-this-workshop.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"About this workshop","heading":"0.3.1.2.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"about-this-workshop.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"About this workshop","heading":"0.3.1.2.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"about-this-workshop.html","id":"linux","chapter":"About this workshop","heading":"0.3.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure least R 3.3.1.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"mediation.html","id":"mediation","chapter":"1 Causal Mediation Analysis","heading":"1 Causal Mediation Analysis","text":"[FILL ]","code":""},{"path":"intro.html","id":"intro","chapter":"2 The Roadmap for Statistical Learning","heading":"2 The Roadmap for Statistical Learning","text":"","code":""},{"path":"intro.html","id":"learning-objectives","chapter":"2 The Roadmap for Statistical Learning","heading":"Learning Objectives","text":"end chapter able :Translate scientific questions statistical questions.Define statistical model based knowledge experiment \ngenerated data.Identify causal parameter function observed data distribution.Explain following causal statistical assumptions \nimplications: ..d., consistency, interference, positivity, SUTVA.","code":""},{"path":"intro.html","id":"introduction","chapter":"2 The Roadmap for Statistical Learning","heading":"Introduction","text":"roadmap statistical learning concerned translation \nreal-world data applications mathematical statistical formulation \nrelevant estimation problem. involves data random variable \nprobability distribution, scientific knowledge represented statistical\nmodel, statistical target parameter representing answer question \ninterest, notion estimator sampling distribution \nestimator.","code":""},{"path":"intro.html","id":"roadmap","chapter":"2 The Roadmap for Statistical Learning","heading":"2.1 The Roadmap","text":"Following roadmap process five stages.Data random variable probability distribution, \\(O \\sim P_0\\).statistical model \\(\\mathcal{M}\\) \\(P_0 \\\\mathcal{M}\\).statistical target parameter \\(\\Psi\\) estimand \\(\\Psi(P_0)\\).estimator \\(\\hat{\\Psi}\\) estimate \\(\\hat{\\Psi}(P_n)\\).measure uncertainty estimate \\(\\hat{\\Psi}(P_n)\\).","code":""},{"path":"intro.html","id":"data-a-random-variable-with-a-probability-distribution-o-sim-p_0","chapter":"2 The Roadmap for Statistical Learning","heading":"(1) Data: A random variable with a probability distribution, \\(O \\sim P_0\\)","text":"data set ’re confronted result experiment can\nview data random variable, \\(O\\), repeat experiment\ndifferent realization experiment. particular, \nrepeat experiment many times learn probability distribution,\n\\(P_0\\), data. , observed data \\(O\\) probability distribution\n\\(P_0\\) \\(n\\) independent identically distributed (..d.) observations \nrandom variable \\(O; O_1, \\ldots, O_n\\). Note data ..d.,\nways handle non-..d. data, establishing conditional\nindependence, stratifying data create sets identically distributed data,\netc. crucial researchers absolutely clear actually\nknow data-generating distribution given problem interest.\nUnfortunately, communication statisticians researchers often\nfraught misinterpretation. roadmap provides mechanism \nensure clear communication research statistician – truly helps\ncommunication!","code":""},{"path":"intro.html","id":"the-empirical-probability-measure-p_n","chapter":"2 The Roadmap for Statistical Learning","heading":"The empirical probability measure, \\(P_n\\)","text":"\\(n\\) ..d. observations empirical probability\nmeasure, \\(P_n\\). empirical probability measure approximation \ntrue probability measure \\(P_0\\), allowing us learn data. \nexample, can define empirical probability measure set, \\(\\), \nproportion observations end \\(\\). ,\n\\[\\begin{equation*}\n  P_n() = \\frac{1}{n}\\sum_{=1}^{n} \\mathbb{}(O_i \\)\n\\end{equation*}\\]order start learning something, need ask “know \nprobability distribution data?” brings us Step 2.","code":""},{"path":"intro.html","id":"the-statistical-model-mathcalm-such-that-p_0-in-mathcalm","chapter":"2 The Roadmap for Statistical Learning","heading":"(2) The statistical model \\(\\mathcal{M}\\) such that \\(P_0 \\in \\mathcal{M}\\)","text":"statistical model \\(\\mathcal{M}\\) defined question asked \nend Step 1. defined set possible probability\ndistributions observed data. Often \\(\\mathcal{M}\\) large (possibly\ninfinite-dimensional), reflect fact statistical knowledge \nlimited. case \\(\\mathcal{M}\\) infinite-dimensional, deem \nnonparametric statistical model.Alternatively, probability distribution data hand described\nfinite number parameters, statistical model parametric. \ncase, subscribe belief random variable \\(O\\) \nobserved , example, normal distribution mean \\(\\mu\\) variance\n\\(\\sigma^2\\). Formally, parametric model may defined\n\\[\\begin{equation*}\n  \\mathcal{M} = \\{P_{\\theta} : \\theta \\\\mathcal{R}^d \\}\n\\end{equation*}\\]Sadly, assumption data-generating distribution specific,\nparametric form common, especially since leap faith \nassumption made convenience. practice oversimplification \ncurrent culture data analysis typically derails attempt trying \nanswer scientific question hand; alas, statements \never-popular quip Box “models wrong useful”\nencourage data analyst make arbitrary choices even practice\noften forces starkly different answers estimation problem. \nTargeted Learning paradigm suffer bias since defines \nstatistical model representation true data-generating\ndistribution corresponding observed data.Now, Step 3: “trying learn data?”","code":""},{"path":"intro.html","id":"the-statistical-target-parameter-psi-and-estimand-psip_0","chapter":"2 The Roadmap for Statistical Learning","heading":"(3) The statistical target parameter \\(\\Psi\\) and estimand \\(\\Psi(P_0)\\)","text":"statistical target parameter, \\(\\Psi\\), defined mapping \nstatistical model, \\(\\mathcal{M}\\), parameter space (.e., real number)\n\\(\\mathcal{R}\\). , \\(\\Psi: \\mathcal{M}\\rightarrow\\mathbb{R}\\). estimand\nmay seen representation quantity wish learn \ndata, answer well-specified (often causal) question interest. \ncontrast purely statistical estimands, causal estimands require\nidentification observed data, based causal models include\nseveral untestable assumptions, described detail section \ncausal target parameters.simple example, consider data set contains observations \nsurvival time every subject, question interest “’s\nprobability someone lives longer five years?” ,\n\\[\\begin{equation*}\n  \\Psi(P_0) = \\mathbb{P}(O > 5)\n\\end{equation*}\\]answer question estimand, \\(\\Psi(P_0)\\), \nquantity ’re trying learn data. defined \\(O\\),\n\\(\\mathcal{M}\\) \\(\\Psi(P_0)\\) formally defined statistical\nestimation problem.","code":""},{"path":"intro.html","id":"the-estimator-hatpsi-and-estimate-hatpsip_n","chapter":"2 The Roadmap for Statistical Learning","heading":"(4) The estimator \\(\\hat{\\Psi}\\) and estimate \\(\\hat{\\Psi}(P_n)\\)","text":"obtain good approximation estimand, need estimator, \npriori-specified algorithm defined mapping set possible\nempirical distributions, \\(P_n\\), live non-parametric statistical\nmodel, \\(\\mathcal{M}_{NP}\\) (\\(P_n \\\\mathcal{M}_{NP}\\)), parameter space\nparameter interest. , \\(\\hat{\\Psi} : \\mathcal{M}_{NP} \\rightarrow \\mathbb{R}^d\\). estimator function takes input\nobserved data, realization \\(P_n\\), gives output value \nparameter space, estimate, \\(\\hat{\\Psi}(P_n)\\).estimator may seen operator maps observed data \ncorresponding empirical distribution value parameter space, \nnumerical output produced function estimate. Thus, \nelement parameter space based empirical probability distribution\nobserved data. plug realization \\(P_n\\) (based sample\nsize \\(n\\) random variable \\(O\\)), get back estimate \\(\\hat{\\Psi}(P_n)\\)\ntrue parameter value \\(\\Psi(P_0)\\).order quantify uncertainty estimate target parameter\n(.e., construct statistical inference), understanding sampling\ndistribution estimator necessary. brings us Step 5.","code":""},{"path":"intro.html","id":"a-measure-of-uncertainty-for-the-estimate-hatpsip_n","chapter":"2 The Roadmap for Statistical Learning","heading":"(5) A measure of uncertainty for the estimate \\(\\hat{\\Psi}(P_n)\\)","text":"Since estimator \\(\\hat{\\Psi}\\) function empirical\ndistribution \\(P_n\\), estimator random variable sampling\ndistribution. , repeat experiment drawing \\(n\\) observations \nevery time end different realization estimate \nestimator sampling distribution. sampling distribution estimators\ncan theoretically validated approximately normally distributed \nCentral Limit Theorem (CLT).class Central Limit Theorems (CLTs) statements regarding \nconvergence sampling distribution estimator normal\ndistribution. general, construct estimators whose limit sampling\ndistributions may shown approximately normal distributed sample size\nincreases. large enough \\(n\\) ,\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\sim N \\left(\\Psi(P_0), \\frac{\\sigma^2}{n}\\right),\n\\end{equation*}\\]\npermitting statistical inference. Now, can proceed quantify \nuncertainty chosen estimator construction hypothesis tests \nconfidence intervals. example, may construct confidence interval \nlevel \\((1 - \\alpha)\\) estimand, \\(\\Psi(P_0)\\):\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\pm z_{1 - \\frac{\\alpha}{2}}\n    \\left(\\frac{\\sigma}{\\sqrt{n}}\\right),\n\\end{equation*}\\]\n\\(z_{1 - \\frac{\\alpha}{2}}\\) \\((1 - \\frac{\\alpha}{2})^\\text{th}\\)\nquantile standard normal distribution. Often, interested \nconstructing 95% confidence intervals, corresponding mass \\(\\alpha = 0.05\\) \neither tail limit distribution; thus, typically take\n\\(z_{1 - \\frac{\\alpha}{2}} \\approx 1.96\\).Note: typically estimate standard error,\n\\(\\frac{\\sigma}{\\sqrt{n}}\\).95% confidence interval means take 100 different samples\nsize \\(n\\) compute 95% confidence interval sample, \napproximately 95 100 confidence intervals contain estimand,\n\\(\\Psi(P_0)\\). practically, means 95% probability\nconfidence interval procedure generates intervals containing \ntrue estimand value (95% confidence “covering” true value). ,\nsingle estimated confidence interval either contain true estimand\n(also called “coverage”).","code":""},{"path":"intro.html","id":"roadmap-summary","chapter":"2 The Roadmap for Statistical Learning","heading":"2.2 Summary of the Roadmap","text":"Data, \\(O\\), viewed random variable probability distribution.\noften \\(n\\) units independent identically distributed units \nprobability distribution \\(P_0\\), \\(O_1, \\ldots, O_n \\sim P_0\\). \nstatistical knowledge experiment generated data. \nwords, make statement true data distribution \\(P_0\\) falls \ncertain set called statistical model, \\(\\mathcal{M}\\). Often sets \nlarge statistical knowledge limited - hence, statistical\nmodels often infinite dimensional models. statistical query , “\ntrying learn data?” denoted statistical target\nparameter, \\(\\Psi\\), maps \\(P_0\\) estimand, \\(\\Psi(P_0)\\). \npoint statistical estimation problem formally defined now \nneed statistical theory guide us construction estimators. ’s \nlot statistical theory review course , particular,\nrelies Central Limit Theorem, allowing us come estimators \napproximately normally distributed also allowing us come \nstatistical inference (.e., confidence intervals hypothesis tests).","code":""},{"path":"intro.html","id":"causal","chapter":"2 The Roadmap for Statistical Learning","heading":"2.3 Causal Target Parameters","text":"many cases, interested problems ask questions regarding \neffect intervention future outcome interest. questions can\nrepresented causal estimands.","code":""},{"path":"intro.html","id":"the-causal-model","chapter":"2 The Roadmap for Statistical Learning","heading":"The Causal Model","text":"formalizing data statistical model, can define causal\nmodel express causal parameters interest. Directed acyclic graphs (DAGs)\none useful tool express know causal relations among\nvariables. Ignoring exogenous \\(U\\) terms (explained ), assume \nfollowing ordering variables observed data \\(O\\). \nusing DAGitty (Textor, Hardt, Knüppel 2011):directed acyclic graphs (DAGs) like provide convenient means \nvisualize causal relations variables, causal relations\namong variables can represented via set structural equations, \ndefine non-parametric structural equation model (NPSEM):\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Y &= f_Y(W, , U_Y),\n\\end{align*}\\]\n\\(U_W\\), \\(U_A\\), \\(U_Y\\) represent unmeasured exogenous background\ncharacteristics influence value variable. NPSEM, \\(f_W\\),\n\\(f_A\\) \\(f_Y\\) denote variable (\\(W\\), \\(\\) \\(Y\\), respectively)\nfunction parents unmeasured background characteristics, note\nimposition particular functional constraints(e.g.,\nlinear, logit-linear, one interaction, etc.). reason, \ncalled non-parametric structural equation models (NPSEMs). DAG set \nnonparametric structural equations represent exactly information \nmay used interchangeably.first hypothetical experiment consider assigning exposure \nwhole population observing outcome, assigning exposure \nwhole population observing outcome. nonparametric structural\nequations, corresponds comparison outcome distribution \npopulation two interventions:\\(\\) set \\(1\\) individuals, \\(\\) set \\(0\\) individuals.interventions imply two new nonparametric structural equation models. \ncase \\(= 1\\), \n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 1 \\\\\n  Y(1) &= f_Y(W, 1, U_Y),\n\\end{align*}\\]\ncase \\(=0\\),\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 0 \\\\\n  Y(0) &= f_Y(W, 0, U_Y).\n\\end{align*}\\]equations, \\(\\) longer function \\(W\\) \nintervened system, setting \\(\\) deterministically either values\n\\(1\\) \\(0\\). new symbols \\(Y(1)\\) \\(Y(0)\\) indicate outcome variable \npopulation generated respective NPSEMs ; \noften called counterfactuals (since run contrary--fact). difference\nmeans outcome two interventions defines \nparameter often called “average treatment effect” (ATE), denoted\n\\[\\begin{equation}\\label{eqn:ate}\n  ATE = \\mathbb{E}_X(Y(1)-Y(0)),\n\\end{equation}\\]\n\\(\\mathbb{E}_X\\) mean theoretical (unobserved) full data\n\\(X = (W, Y(1), Y(0))\\).Note, can define much complicated interventions NPSEM’s, \ninterventions based upon rules (based upon covariates), stochastic\nrules, etc. results different targeted parameter entails\ndifferent identifiability assumptions discussed .","code":"\nlibrary(dagitty)\nlibrary(ggdag)\n\n# make DAG by specifying dependence structure\ndag <- dagitty(\n  \"dag {\n    W -> A\n    W -> Y\n    A -> Y\n    W -> A -> Y\n  }\"\n)\nexposures(dag) <- c(\"A\")\noutcomes(dag) <- c(\"Y\")\ntidy_dag <- tidy_dagitty(dag)\n\n# visualize DAG\nggdag(tidy_dag) +\n  theme_dag()"},{"path":"intro.html","id":"identifiability","chapter":"2 The Roadmap for Statistical Learning","heading":"Identifiability","text":"can never observe \\(Y(0)\\) (counterfactual outcome \\(=0\\))\n\\(Y(1)\\) (similarly, counterfactual outcome \\(=1\\)), \nestimate  directly. Instead, make assumptions \nquantity may estimated observed data \\(O \\sim P_0\\) \ndata-generating distribution \\(P_0\\). Fortunately, given causal model\nspecified NPSEM , can, handful untestable assumptions,\nestimate ATE, even observational data. assumptions may \nsummarized follows.causal graph implies \\(Y() \\perp \\) \\(\\\\mathcal{}\\), \nrandomization assumption. case observational data, \nanalogous assumption strong ignorability unmeasured confounding\n\\(Y() \\perp \\mid W\\) \\(\\\\mathcal{}\\);Although represented causal graph, also required assumption\ninterference units, , outcome unit \\(\\) \\(Y_i\\) \naffected exposure unit \\(j\\) \\(A_j\\) unless \\(=j\\);Consistency treatment mechanism also required, .e., outcome\nunit \\(\\) \\(Y_i()\\) whenever \\(A_i = \\), assumption also known “\nversions treatment”;also necessary observed units, across strata defined \\(W\\),\nbounded (non-deterministic) probability receiving treatment –\n, \\(0 < \\mathbb{P}(= \\mid W) < 1\\) \\(\\) \\(W\\)). assumption\nreferred positivity overlap.Remark: Together, (2) (3), assumptions interference \nconsistency, respectively, jointly referred stable unit\ntreatment value assumption (SUTVA).Given assumptions, ATE may re-written function \\(P_0\\),\nspecifically\n\\[\\begin{equation}\\label{eqn:estimand}\n  ATE = \\mathbb{E}_0(Y(1) - Y(0)) = \\mathbb{E}_0\n    \\left(\\mathbb{E}_0[Y \\mid = 1, W] - \\mathbb{E}_0[Y \\mid = 0, W]\\right).\n\\end{equation}\\]\nwords, ATE difference predicted outcome values subject, \ncontrast treatment conditions (\\(= 0\\) vs. \\(= 1\\)), population,\naveraged observations. Thus, parameter theoretical “full” data\ndistribution can represented estimand observed data\ndistribution. Significantly, nothing representation \n requires parameteric assumptions; thus, regressions\nright hand side may estimated freely machine learning. \ndifferent parameters, potentially different identifiability\nassumptions resulting estimands can functions different components\n\\(P_0\\). discuss several complex estimands later sections \nhandbook.","code":""},{"path":"natural.html","id":"natural","chapter":"3 The Natural Direct and Indirect Effects","heading":"3 The Natural Direct and Indirect Effects","text":"[FILL ]","code":""},{"path":"interventional.html","id":"interventional","chapter":"4 The Interventional Direct and Indirect Effects","heading":"4 The Interventional Direct and Indirect Effects","text":"[FILL ]","code":""},{"path":"stochastic.html","id":"stochastic","chapter":"5 The Stochastic Direct and Indirect Effects","heading":"5 The Stochastic Direct and Indirect Effects","text":"[FILL ]","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Textor, Johannes, Juliane Hardt, Sven Knüppel. 2011. “DAGitty: Graphical Tool Analyzing Causal Diagrams.” Epidemiology 22 (5): 745.","code":""}]
